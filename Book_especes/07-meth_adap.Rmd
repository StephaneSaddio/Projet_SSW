## Méthodes adaptatives 

  \hspace*{0.5cm} On introduit précédemment la notion de l'estimation de la densité qui dépend d'un paramètre de lissage $h$. Soit  $(\hat{f_h})_{h\in \mathcal H}$ une famille des estimateurs de la fonction densité $f$ que que l'on cherchons cherche à estimer.\newline
Une question s'impose : comment peut-on construire un estimateur à risque optimal à partir de cette famille en tenant en considération les observations ? \newline
Dans la théorie adaptative f est toujours supposée appartenir à une classe fonctionnelle. Cette classe n'est pas connu à priori mais supposé appartenir à une famille de classes fonctionnelles{$\mathcal{F_{\alpha}},\alpha \in\mathcal{A}$} où $\mathcal{A}$ est un ensemble des paramètres de nuisance. ( ref : Sur l’estimation adaptative d’une densité multivariée sous l’hypothèse de la structure d’indépendance-Approche minimax adaptative).\newline

  \hspace*{0.5cm}  Dans cette partie, afin de répondre à la question posée auparavant, nous allons discuter du choix du noyau en premier lieu. Ensuite, nous introduirons deux méthodes pour le choix du paramètre de lissage $h$.

 
### Choix du noyau

  \hspace*{0.5cm} Avant de présenter le critère de choix du noyau nous allons introduire quelques outils mathématiques qui simplifient l'écriture du critère.  
  Tout d'abord, nous avons besoin du risque quadratique $\mathcal R$ aussi appelé l'erreur quadratique moyenne (**M**ean **S**quared **E**rror en anglais). Dans cette partie nous allons noter $MSE$ le risque quadratique pour des questions pratiques.  
  
$$
\begin{aligned}
MSE &= \mathcal R = \mathbb E \left[ \{  \hat f_n(x)-f(x) \} \right] \\
&=\mathbb V \left[ \hat f_n(x) \right] + Biais^2 \left[ \hat f_n(x) \right] \\
&= MSE(x ; n, h, K, f).
\end{aligned}
$$
comme nous l'avons montré précédemment. \@ref{#holder}  

\begin{propri} (Biais ponctuel)
Pour tout $x$ fixé dans $\mathbb R$, le biais de l'estimateur $\hat f_n$ est donnée par l'équation suivante : 
$$Biais \left[ \hat f_n(x) \right] \approx \frac12h^2f''(x)\int_{\mathbb R} t^2K(t) dt.$$
\end{propri}

\begin{demo}
Démontrons la propriété du biais.
Rappelons que les variables aléatoires $\{ X_1,~\dots, ~X_n\}$ sont $i.i.d.$, alors
$$
\begin{aligned}
Biais  \left[ \hat f_n(x) \right] &= \mathbb E \left[ \hat f_n(x) \right] - f(x) \\
&= \mathbb E \left[ \frac1{nh} \sum\limits_{i=1}^n K\left(\frac{X_i-x}h\right) - f(x) \right] \\
&= \frac1{nh} \sum\limits_{i=1}^n \mathbb E \left[  K\left(\frac{X_i-x}h\right) \right] - f(x) ~\text{par linéarité de l'espérance} \\
&= \frac1{h} \mathbb E \left[  K\left(\frac{X_1-x}h\right) \right] - f(x) ~~~~~\text{car les } X_i \text{ sont }i.i.d.\\
&= \frac1{h} \int_{\mathbb R}  K\left(\frac{x_1-x}h\right) f(x_1)dx_1 - f(x).
\end{aligned}
$$

Procédons à un changement de variables.
$$t=\frac{x_1-x}h ~~~~\hbox{et}~~~~dt =\frac{dx_1}h.$$
On obtient alors, 
$$
Biais  \left[ \hat f_n(x) \right]
= \int_{\mathbb R}  K(t)  f(x+ht)dt - f(x).
$$

Après avoir transformé l'écriture du biais, nous pouvons donner une approximation de celui-ci en utilisant la formule de Taylor-Young à l'ordre 2.
$$f(x+ht)=f(x)+htf'(x)+\frac{h^2t^2}2 f''(x)+o(h^2t^2).$$
Ce qui nous donne en remplaçant dans la formule
$$
Biais \left[ \hat f_n(x) \right] \approx \frac12h^2f''(x)\int_{\mathbb R} t^2K(t) dt - f(x) + o(h^2t^2).
$$

et puisque le noyau est centré et son intégrale sur $\mathbb R$ est égale à $1$, on a bien que 
$$Biais \left[ \hat f_n(x) \right] \approx \frac12h^2f''(x)\int_{\mathbb R} t^2K(t) dt.$$

\end{demo}

\begin{propri} (Variance ponctuelle)
Pour tout $x$ fixé dans $\mathbb R$, la variance de l'estimateur $\hat f_n$ est donnée par l'équation suivante : 
$$\mathbb V \left[ \hat f_n(x) \right] \approx \frac1{nh} f(x) \int_{\mathbb R}K(t)^2 dt.$$
\end{propri}

\begin{demo}
Démontrons la propriété de la variance.
Rappelons que les variables aléatoires $\{ X_1,~\dots, ~X_n\}$ sont $i.i.d.$, alors
$$
\begin{aligned}
\mathbb V  \left[ \hat f_n(x) \right] &= \mathbb V \left[ \frac1{nh} \sum\limits_{i=1}^n K\left(\frac{X_i-x}h\right) \right] \\
&= \frac1{nh^2} \mathbb V \left[  K\left(\frac{X_1-x}h\right) \right] ~~~~~\text{car les } X_i \text{ sont }i.i.d.\\
&= \frac1{nh^2} \mathbb E \left[  K\left(\frac{X_1-x}h\right)^2 \right] - \frac1{nh^2} \mathbb E \left[  K\left(\frac{X_1-x}h\right) \right]^2\\
&= \frac1{nh^2} \int_{\mathbb R}  K^2\left(\frac{x_1-x}h\right) f(x_1)dx_1 - \frac1{nh^2} \left[\int_{\mathbb R}  K\left(\frac{x_1-x}h\right) f(x_1)dx_1\right]^2.
\end{aligned}
$$
On effectue encore une fois le même changement de variables.
$$t=\frac{x_1-x}h ~~~~\hbox{et}~~~~dt =\frac{dx_1}h.$$
Nous trouvons désormais,
$$
\begin{aligned}
\mathbb V  \left[ \hat f_n(x) \right] &= \frac1{nh} \int_{\mathbb R}  K^2\left(t\right) f(x+ht)dt - \frac1{n} \left[\int_{\mathbb R}  K\left(t\right) f(x+ht)dt\right]^2\\
&= \frac1{nh} \int_{\mathbb R}  K^2\left(t\right) f(x+ht)dt - \frac1{n} \left[Biais \left( \hat f_n(x) \right) +f(x)\right]^2\\
&= \frac1{nh} \int_{\mathbb R}  K^2\left(t\right) f(x+ht)dt - \frac1{n} \left[O \left( h^2 \right) +f(x)\right]^2.
\end{aligned}
$$
Donc si la condition $\int_{mathbb R}K(u)^2du<+\infty$ est vérifiée et que la taille de l'échantillon est importante, l'équation suivante est avérée : 
$$\mathbb V \left[ \hat f_n(x) \right] \approx \frac1{nh} f(x) \int_{\mathbb R}K(t)^2 dt.$$

\end{demo}

L'approximation de l'erreur quadratique (**A**verage of **M**ean **S**quared **E**rror en anglais) est donnée par l'équation suivante :  
$$
AMSE(x)= \frac1{nh} f(x) \int_{\mathbb R}K(t)^2 dt + \left(\frac12h^2f''(x)\int_{\mathbb R} t^2K(t) dt \right)^2.
$$
Elle a été calculée à partir de la variance approchée et du biais approché.


L'erreur quadratique moyenne intégrée (**M**ean **I**ntregrated **S**quared **E**rror en anglais) est une mesure théorique communément utilisée pour évaluer la différence entre $f$ et $\hat f_n$. Pour l'évaluer on utilise l'erreur quadratique moyenne qu'on intègre sur le support $\mathbb R$ de l'estimateur.
$$
\begin{aligned}
MISE(n,h,K,f) &= \int_{\mathbb R}MSE(x;n,h,K,f)dx\\
&= \int_{\mathbb R} \mathbb V\left[ \hat f_n(x) \right]  dx+ \int_{\mathbb R} Biais^2 \left[ \hat f_n(x) \right]dx
\end{aligned}
$$

De la même façon que nous l'avons fait avec l'erreur quadratique moyenne, nous allons calculer l'expression approchée de l'erreur quadratique moyenne (**A**verage of **M**ean **I**ntegrated **S**quared **E**rror en anglais).

$$
\begin{aligned}
AMISE(x)&= \frac1{nh} \int_{\mathbb R}f(x)dx \int_{\mathbb R}K(t)^2 dt + \frac14h^4 \int_{\mathbb R}f''(x)dx ~~\left(\int_{\mathbb R} t^2K(t) dt \right)^2\\
&= \frac1{nh} \int_{\mathbb R}K(t)^2 dt 
+ \frac14h^4 \int_{\mathbb R}f''(x)dx ~~\left(\int_{\mathbb R} t^2K(t) dt \right)^2\\
&= \frac1{nh} \int_{\mathbb R}K(t)^2 dt 
+ \frac14h^4 ~ \mathbb V (K)^2 \int_{\mathbb R}f''(x)dx 
\end{aligned}
$$

 \hspace*{0.5cm}  à présent que nous avons défini les outils nécessaires au choix du noyau, nous allons pouvoir présenter un critère de choix pour les noyaux continus symétriques. Afin de mesurer l'efficacité des noyaux, nous utilisons une mesure qui calcule le rapport du critère $AMISE$ de deux noyaux.
$$
eff(K_1,K_2) = \frac{AMISE(K_1)}{AMISE(K_2)}.
$$
Supposons que $K_1$ est le noyau d'Epanechnikov, il est souvent utilisé comme référence par rapport aux autres noyaux continus. Après quelques calculs, on obtient que l'efficacité d'un noyau K par rapport à celui d'Epanechnikov est donnée par 
$$
eff(K) = \frac{3}{5\times \int_{\mathbb R}K(t)^2 dt\sqrt{5 \times\int_{\mathbb R}t^2K(t) dt} } \leq1.
$$
Voici un tableau récapitulatif de l'efficacité  de plusieurs noyaux continus symétriques.

```{r ker_tab, echo = F}
dt <- data.frame(Noyau = c("Epanechnikov", "Bigweight", "Triangular",
"Gaussien", "Rectangulaire"), Efficacité = c(1.000, 0.994, 0.986, 0.951, 0.930))
knitr::kable(dt, caption = "Efficacité des noyaux continus symétriques")
```

\begin{rem}
Les valeurs d'efficacité des noyaux sont très proches les unes des autres dans le cas étudié, surtout pour les trois premiers noyaux du tableau. c'est pour cela que le choix du noyau n'a au final que peu d'importance dans l'estimation de la densité. 
\end{rem}

#### Comment choisir les paramètres de la méthode ?

  Dans la méthode d'estimation à noyau le choix du noyau n'est pas le plus important, le vrai enjeu de cette méthode est le choix de la fenêtre $h$ (*bandwidth*).
  En effet, la fenêtre détermine l'influence des données dans l'estimation. Si $h$ est petit, l'effet local est important donc on aura beaucoup de bruit. Si $h$ est grand, on aura une estimation plus douce, plus lisse.
  
  Nous pouvons constater l'influence du paramètre $h$ sur l'exemple suivant :

  Nous avons simulé 500 variables suivant une loi de Weibull de paramètres ($\alpha = 1.7$, $\lambda=2$) représentées dans l'histogramme. La courbe en rouge est la fonction de densité de la loi de Weibull et la bleue est l'estimation avec la méthode des noyaux sur les variables simulées.
```{r func_Risk, echo =F}
# Fonction calculant le risque quadratique (MSE)
Risk<- function(sample, bandwith, kernel){
  
  X  <- sample
  n <- length(X)
  x <- seq.int(min(X), max(X), length.out = n)
  if (missing(kernel)) {kernel = "gaussian"}
  kern <- switch(kernel,
                 gaussian = function(u){return(dnorm(u))},
                 rectangular = function(u){
                   ifelse(abs(u) < 1, .5, 0) },
                 triangular = function(u) {
                   return(ifelse(abs(u) < 1, (1 - abs(u)), 0)) },
                 epanechnikov = function(u){
                   return(ifelse(abs(u) < 1, 3/4*(1 - u^2), 0)) },
                 biweight = function(u){ 
                   return(ifelse(abs(u) < 1, 15/16*(1 - u^2)^2, 0)) }) # noyau caractère en fonction
  
  hatf <- function(x, sample, h, kernel){
    n <- length(sample)
    som <- 0
    for (i in 1:n){
     som <- som + kern((sample[i]-x)/h) 
    }
    return(som/(n*h))
  } 
   
  s <- 0
  for (i in 1:n){ 
    s <- s + sum(kern( (X[i]-X[-i])/bandwith)) 
  }
  

  n2hat_f <- integrate(function(x) hatf(x, sample = X, h = bandwith, kernel = kern), min(X)-50, max(X)+50)$value ^2

  R <- n2hat_f - 2/(n*(n-1))* s /bandwith

  return(R)
}

```

```{r, echo=F}
par(mfrow=c(2,2))

seq <- seq(0,6, length.out = 40)
yweib <- dweibull(seq,1.7,2)
rand <-rweibull(500,1.7,2)

hist(rand, breaks = 12, freq = F, main = "")
lines(density(rand, bw = 0.1), col ="blue")
lines(density(rand), col ="blue")
lines(density(rand, bw = 2), col ="blue")
lines(seq, yweib, col="red")
title("Fenêtre trop petite,\n h = 0.1")

hist(rand, breaks = 12, freq = F, main = "")
lines(density(rand), col ="blue")
lines(seq, yweib, col="red")
title("Fenêtre raisonnable,\n h = 0.372")

hist(rand, breaks = 12, freq = F, main = "")
lines(density(rand, bw = 2), col ="blue")
lines(seq, yweib, col="red")
title("Fenêtre trop petite,\n h = 2")
```
La fenêtre $h$ du second graphique est calculée automatiquement par la fonction `density`de R. 
 