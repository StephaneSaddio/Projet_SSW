<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Estimateur de densité à noyau | Rapport du projet de M1</title>
  <meta name="description" content="Chapter 3 Estimateur de densité à noyau | Rapport du projet de M1" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Estimateur de densité à noyau | Rapport du projet de M1" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Estimateur de densité à noyau | Rapport du projet de M1" />
  
  
  

<meta name="author" content="Wiam Chaoui" />
<meta name="author" content="Sophie Manuel" />
<meta name="author" content="Stéphane Sadio" />


<meta name="date" content="2021-01-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="méthodes-non-paramétriques.html"/>
<link rel="next" href="applications.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#problématique"><i class="fa fa-check"></i><b>1.2</b> Problématique</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="méthodes-non-paramétriques.html"><a href="méthodes-non-paramétriques.html"><i class="fa fa-check"></i><b>2</b> Méthodes non-paramétriques</a><ul>
<li class="chapter" data-level="2.1" data-path="méthodes-non-paramétriques.html"><a href="méthodes-non-paramétriques.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="méthodes-non-paramétriques.html"><a href="méthodes-non-paramétriques.html#estimation-par-histogramme"><i class="fa fa-check"></i><b>2.2</b> Estimation par histogramme</a></li>
<li class="chapter" data-level="2.3" data-path="méthodes-non-paramétriques.html"><a href="méthodes-non-paramétriques.html#estimateurs-par-projection"><i class="fa fa-check"></i><b>2.3</b> Estimateurs par projection :</a></li>
<li class="chapter" data-level="2.4" data-path="méthodes-non-paramétriques.html"><a href="méthodes-non-paramétriques.html#estimateurs-à-noyau-de-densité"><i class="fa fa-check"></i><b>2.4</b> Estimateurs à noyau de densité</a><ul>
<li class="chapter" data-level="2.4.1" data-path="méthodes-non-paramétriques.html"><a href="méthodes-non-paramétriques.html#comment-construit-on-un-estimateur-à-noyau"><i class="fa fa-check"></i><b>2.4.1</b> Comment construit-on un estimateur à noyau ?</a></li>
<li class="chapter" data-level="2.4.2" data-path="méthodes-non-paramétriques.html"><a href="méthodes-non-paramétriques.html#quest-un-noyau"><i class="fa fa-check"></i><b>2.4.2</b> Qu’est un noyau ?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="estimateur-de-densité-à-noyau.html"><a href="estimateur-de-densité-à-noyau.html"><i class="fa fa-check"></i><b>3</b> Estimateur de densité à noyau</a><ul>
<li class="chapter" data-level="3.1" data-path="estimateur-de-densité-à-noyau.html"><a href="estimateur-de-densité-à-noyau.html#evaluer-un-estimateur"><i class="fa fa-check"></i><b>3.1</b> Evaluer un estimateur</a><ul>
<li class="chapter" data-level="3.1.1" data-path="estimateur-de-densité-à-noyau.html"><a href="estimateur-de-densité-à-noyau.html#risque-quadratique-des-estimateurs-à-noyau-sur-les-classe-des-espaces-de-hölder"><i class="fa fa-check"></i><b>3.1.1</b> Risque quadratique des estimateurs à noyau sur les classe des espaces de Hölder</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="estimateur-de-densité-à-noyau.html"><a href="estimateur-de-densité-à-noyau.html#méthodes-adaptatives"><i class="fa fa-check"></i><b>3.2</b> Méthodes adaptatives</a><ul>
<li class="chapter" data-level="3.2.1" data-path="estimateur-de-densité-à-noyau.html"><a href="estimateur-de-densité-à-noyau.html#choix-du-noyau"><i class="fa fa-check"></i><b>3.2.1</b> Choix du noyau</a></li>
<li class="chapter" data-level="3.2.2" data-path="estimateur-de-densité-à-noyau.html"><a href="estimateur-de-densité-à-noyau.html#comment-choisir-les-paramètres-de-la-méthode"><i class="fa fa-check"></i><b>3.2.2</b> Comment choisir les paramètres de la méthode ?</a></li>
<li class="chapter" data-level="3.2.3" data-path="estimateur-de-densité-à-noyau.html"><a href="estimateur-de-densité-à-noyau.html#choix-de-la-fenêtre"><i class="fa fa-check"></i><b>3.2.3</b> Choix de la fenêtre</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>4</b> Applications</a><ul>
<li class="chapter" data-level="4.1" data-path="applications.html"><a href="applications.html#fonction-dens"><i class="fa fa-check"></i><b>4.1</b> Fonction dens</a></li>
<li class="chapter" data-level="4.2" data-path="applications.html"><a href="applications.html#applications-aux-données-du-vivant"><i class="fa fa-check"></i><b>4.2</b> Applications aux données du vivant</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="6" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>6</b> References</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Rapport du projet de M1</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimateur-de-densité-à-noyau" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Estimateur de densité à noyau</h1>
<div id="evaluer-un-estimateur" class="section level2">
<h2><span class="header-section-number">3.1</span> Evaluer un estimateur</h2>
<p>Avant de commencer cette partie on va d’abord introduire quelques notions et définitions</p>




<p>Pour évaluer un estimateur <span class="math inline">\(\hat{f}\)</span> on définit son risque associé.</p>



<div id="risque-quadratique-des-estimateurs-à-noyau-sur-les-classe-des-espaces-de-hölder" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Risque quadratique des estimateurs à noyau sur les classe des espaces de Hölder</h3>
<p>Nous nous intéressons au risque quadratique de <span class="math inline">\(\hat{f}_n\)</span>, définit par :<br />
étant donné <span class="math inline">\(x_0 \in \mathbb{R}\)</span>
<span class="math display">\[
R(\hat {f}_n, f) = \mathbb{E}[|\hat {f}_n(x_0) - f(x_0)|^2]
\]</span></p>
<p>Rappelons la décomposition “biais-variance” du risque quadratique :
<span class="math display">\[
\mathbb{E}[|\hat {f}_n(x_0) - f(x_0)|^2] = (\mathbb{E}[\hat {f}_n(x_0)] - f(x_0))^2 + \mathbb{V}[\hat {f}_n(x_0)]
\]</span></p>
<div id="majoration-du-biais-et-de-la-variance" class="section level4">
<h4><span class="header-section-number">3.1.1.1</span> Majoration du biais et de la variance</h4>
<p>Dans cette section, nous allons nous intéresser au compromis biais-variance afin de minimiser le risque quadratique.
Nous introduirons après quelques définitions deux propositions qui montrent que sous certaines hypothèses, on peut majorer le biais ainsi que la variance.</p>





<p>\begin{exem} (Simulation numérique)
Nous estimons la fonction densité d’une somme de deux variables gaussiennes ci-contre avec la méthode à noyau avec différentes fenêtres.
<span class="math display">\[
f(x)=\frac{1}{2}\frac{1}{\sqrt{2\pi}}(exp(-\frac{(x-2)^2}{2})+exp(-\frac{(x-6)^2}{2}))
\]</span>
On va en fait utiliser <code>ggplot</code> pour représenter l’estimateur à noyau. La fonction qui permet de dessiner l’estimateur à noyau est <code>geom_density</code>. Le paramètre représentant le fenêtre h rappelle <code>bw</code> (comme bandwidth en Anglais).
\end{exem}</p>
<p><img src="Book-especes_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>

</div>
</div>
</div>
<div id="méthodes-adaptatives" class="section level2">
<h2><span class="header-section-number">3.2</span> Méthodes adaptatives</h2>
<p> On introduit précédemment la notion de l’estimation de la densité qui dépend d’un paramètre de lissage <span class="math inline">\(h\)</span>. Soit <span class="math inline">\((\hat{f_h})_{h\in \mathcal H}\)</span> une famille des estimateurs de la fonction densité <span class="math inline">\(f\)</span> que que l’on cherchons cherche à estimer.
Une question s’impose : comment peut-on construire un estimateur à risque optimal à partir de cette famille en tenant en considération les observations ? 
Dans la théorie adaptative f est toujours supposée appartenir à une classe fonctionnelle. Cette classe n’est pas connu à priori mais supposé appartenir à une famille de classes fonctionnelles{<span class="math inline">\(\mathcal{F_{\alpha}},\alpha \in\mathcal{A}\)</span>} où <span class="math inline">\(\mathcal{A}\)</span> est un ensemble des paramètres de nuisance. ( ref : Sur l’estimation adaptative d’une densité multivariée sous l’hypothèse de la structure d’indépendance-Approche minimax adaptative).</p>
<p> Dans cette partie, afin de répondre à la question posée auparavant, nous allons discuter du choix du noyau en premier lieu. Ensuite, nous introduirons deux méthodes pour le choix du paramètre de lissage <span class="math inline">\(h\)</span>.</p>
<div id="choix-du-noyau" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Choix du noyau</h3>
<p> Avant de présenter le critère de choix du noyau nous allons introduire quelques outils mathématiques qui simplifient l’écriture du critère.<br />
Tout d’abord, nous avons besoin du risque quadratique <span class="math inline">\(\mathcal R\)</span> aussi appelé l’erreur quadratique moyenne (<strong>M</strong>ean <strong>S</strong>quared <strong>E</strong>rror en anglais). Dans cette partie nous allons noter <span class="math inline">\(MSE\)</span> le risque quadratique pour des questions pratiques.</p>
<p><span class="math display">\[
\begin{aligned}
MSE &amp;= \mathcal R = \mathbb E \left[ \{  \hat f_n(x)-f(x) \} \right] \\
&amp;=\mathbb V \left[ \hat f_n(x) \right] + Biais^2 \left[ \hat f_n(x) \right] \\
&amp;= MSE(x ; n, h, K, f).
\end{aligned}
\]</span>
comme nous l’avons montré précédemment. @ref{#holder}</p>




<p>L’approximation de l’erreur quadratique (<strong>A</strong>verage of <strong>M</strong>ean <strong>S</strong>quared <strong>E</strong>rror en anglais) est donnée par l’équation suivante :<br />
<span class="math display">\[
AMSE(x)= \frac1{nh} f(x) \int_{\mathbb R}K(t)^2 dt + \left(\frac12h^2f&#39;&#39;(x)\int_{\mathbb R} t^2K(t) dt \right)^2.
\]</span>
Elle a été calculée à partir de la variance approchée et du biais approché.</p>
<p>L’erreur quadratique moyenne intégrée (<strong>M</strong>ean <strong>I</strong>ntregrated <strong>S</strong>quared <strong>E</strong>rror en anglais) est une mesure théorique communément utilisée pour évaluer la différence entre <span class="math inline">\(f\)</span> et <span class="math inline">\(\hat f_n\)</span>. Pour l’évaluer on utilise l’erreur quadratique moyenne qu’on intègre sur le support <span class="math inline">\(\mathbb R\)</span> de l’estimateur.
<span class="math display">\[
\begin{aligned}
MISE(n,h,K,f) &amp;= \int_{\mathbb R}MSE(x;n,h,K,f)dx\\
&amp;= \int_{\mathbb R} \mathbb V\left[ \hat f_n(x) \right]  dx+ \int_{\mathbb R} Biais^2 \left[ \hat f_n(x) \right]dx
\end{aligned}
\]</span></p>
<p>De la même façon que nous l’avons fait avec l’erreur quadratique moyenne, nous allons calculer l’expression approchée de l’erreur quadratique moyenne (<strong>A</strong>verage of <strong>M</strong>ean <strong>I</strong>ntegrated <strong>S</strong>quared <strong>E</strong>rror en anglais).</p>
<p><span class="math display">\[
\begin{aligned}
AMISE(x)&amp;= \frac1{nh} \int_{\mathbb R}f(x)dx \int_{\mathbb R}K(t)^2 dt + \frac14h^4 \int_{\mathbb R}f&#39;&#39;(x)dx ~~\left(\int_{\mathbb R} t^2K(t) dt \right)^2\\
&amp;= \frac1{nh} \int_{\mathbb R}K(t)^2 dt 
+ \frac14h^4 \int_{\mathbb R}f&#39;&#39;(x)dx ~~\left(\int_{\mathbb R} t^2K(t) dt \right)^2\\
&amp;= \frac1{nh} \int_{\mathbb R}K(t)^2 dt 
+ \frac14h^4 ~ \mathbb V (K)^2 \int_{\mathbb R}f&#39;&#39;(x)dx 
\end{aligned}
\]</span></p>
<p> à présent que nous avons défini les outils nécessaires au choix du noyau, nous allons pouvoir présenter un critère de choix pour les noyaux continus symétriques. Afin de mesurer l’efficacité des noyaux, nous utilisons une mesure qui calcule le rapport du critère <span class="math inline">\(AMISE\)</span> de deux noyaux.
<span class="math display">\[
eff(K_1,K_2) = \frac{AMISE(K_1)}{AMISE(K_2)}.
\]</span>
Supposons que <span class="math inline">\(K_1\)</span> est le noyau d’Epanechnikov, il est souvent utilisé comme référence par rapport aux autres noyaux continus. Après quelques calculs, on obtient que l’efficacité d’un noyau K par rapport à celui d’Epanechnikov est donnée par
<span class="math display">\[
eff(K) = \frac{3}{5\times \int_{\mathbb R}K(t)^2 dt\sqrt{5 \times\int_{\mathbb R}t^2K(t) dt} } \leq1.
\]</span>
Voici un tableau récapitulatif de l’efficacité de plusieurs noyaux continus symétriques.</p>
<table>
<caption>(#tab:ker_tab)Efficacité des noyaux continus symétriques</caption>
<thead>
<tr class="header">
<th align="left">Noyau</th>
<th align="right">Efficacité</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Epanechnikov</td>
<td align="right">1.000</td>
</tr>
<tr class="even">
<td align="left">Bigweight</td>
<td align="right">0.994</td>
</tr>
<tr class="odd">
<td align="left">Triangular</td>
<td align="right">0.986</td>
</tr>
<tr class="even">
<td align="left">Gaussien</td>
<td align="right">0.951</td>
</tr>
<tr class="odd">
<td align="left">Rectangulaire</td>
<td align="right">0.930</td>
</tr>
</tbody>
</table>

</div>
<div id="comment-choisir-les-paramètres-de-la-méthode" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Comment choisir les paramètres de la méthode ?</h3>
<p>Dans la méthode d’estimation à noyau le choix du noyau n’est pas le plus important, le vrai enjeu de cette méthode est le choix de la fenêtre <span class="math inline">\(h\)</span> (<em>bandwidth</em>).
En effet, la fenêtre détermine l’influence des données dans l’estimation. Si <span class="math inline">\(h\)</span> est petit, l’effet local est important donc on aura beaucoup de bruit. Si <span class="math inline">\(h\)</span> est grand, on aura une estimation plus douce, plus lisse.</p>
<p>Nous pouvons constater l’influence du paramètre <span class="math inline">\(h\)</span> sur l’exemple suivant :</p>
<p>Nous avons simulé 500 variables suivant une loi de Weibull de paramètres (<span class="math inline">\(\alpha = 1.7\)</span>, <span class="math inline">\(\lambda=2\)</span>) représentées dans l’histogramme. La courbe en rouge est la fonction de densité de la loi de Weibull et la bleue est l’estimation avec la méthode des noyaux sur les variables simulées.</p>
<p><img src="Book-especes_files/figure-html/unnamed-chunk-3-1.png" width="672" />
La fenêtre <span class="math inline">\(h\)</span> du second graphique est calculée automatiquement par la fonction <code>density</code>de R.</p>

<div style="page-break-after: always;"></div>
</div>
<div id="choix-de-la-fenêtre" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Choix de la fenêtre</h3>
<p>L’estimation de densité nécessite de faire le choix de la taille de la fenêtre qu’on note <span class="math inline">\(h\)</span>.En statistique non-paramétrique, ils existent plusieurs méthodes et critères de qualité pour le choix de la fenêtre.</p>
<p>On présente dans la suite deux méthodes :
* Méthode de validation croisée 
* Méthode de Goldenshluger-Lepski.</p>
<div id="choix-de-la-fenêtre-h-par-validation-croisée" class="section level4">
<h4><span class="header-section-number">3.2.3.1</span> Choix de la fenêtre <span class="math inline">\(h\)</span> par validation croisée</h4>
<p>Le choix de la fenêtre dans la section précédente est critiquable: comme on l’a mentionné, il dépend de la régularité de la fonction <span class="math inline">\(f\)</span> qui est inconnue dans notre cas. On peut donc essayer d’estimer cette fenêtre idéale par un estimateur <span class="math inline">\(\hat{h}\)</span>. De façon à souligner la dépendance à la fonction, on va noter <span class="math inline">\(\hat{f}_{n,h}\)</span> l’estimateur associé à un choix de fenêtre <span class="math inline">\(h\)</span>. L’estimateur final sera <span class="math inline">\(\hat{f}_{n,\hat{h}}\)</span>, une fois le choix de <span class="math inline">\(\hat{h}\)</span> fait.
On cherche à minimiser en <span class="math inline">\(h\)</span> le risque quadratique pour la distance <span class="math inline">\(L_2\)</span> :
<span class="math display">\[
\begin{aligned}
R(\hat {f}_{n,h})&amp;=\mathbb{E}[\begin{Vmatrix}\hat {f}_{n,h}-f\end{Vmatrix}_2^2]\\        
&amp;= \mathbb{E}[\begin{Vmatrix}\hat {f}_{n,h}\end{Vmatrix}_2^2] -2~\mathbb{E}[\int \hat {f}_{n,h}(x)f(x)dx] +\begin{Vmatrix}f\end{Vmatrix}_2^2
\end{aligned}
\]</span></p>
<p>Or la fonction <span class="math inline">\(f\)</span> étant inconnue, ce risque n’est pas calculable à partir des données. On cherche donc à estimer ce risque en utilisant uniquement les données. Remarquons que minimiser en <span class="math inline">\(h\)</span> la quantité <span class="math inline">\(R(\hat {f}_{n,h}, f)\)</span> est équivalent à minimiser en <span class="math inline">\(h\)</span> la quantité <span class="math inline">\(R(\hat {f}_{n,h}, f)-\begin{Vmatrix}f\end{Vmatrix}_2^2\)</span>. On va en fait remplacer la minimisation de la quantité inconnue <span class="math inline">\(R(\hat {f}_{n,h}, f)-\begin{Vmatrix}f\end{Vmatrix}_2^2\)</span> par la minimisation d’un estimateur <span class="math inline">\(\hat {R}(h)\)</span> de cette quantité. Plus précisément on va chercher un estimateur sans biais de cette expression:</p>
<p><span class="math display">\[
\mathbb{E}[\begin{Vmatrix}\hat {f}_{n,h}\end{Vmatrix}_2^2] -2~\mathbb{E}[\int \hat {f}_{n,h}(x)f(x)dx]
\]</span></p>
<p>Le premier terme admet <span class="math inline">\(\begin{Vmatrix}\hat {f}_{n,h}\end{Vmatrix}_2^2\)</span> comme estimateur trivial (d’après la propriété des estimateurs sans biais : <span class="math inline">\(\mathbb{E}[\hat {\beta}]=\beta\)</span>).
Il reste à trouver un estimateur sans biais du second terme. Pour cela, nous admettons par construction l’estimateur sans biais <span class="math inline">\(\hat {G}\)</span> définit en tout points sauf en <span class="math inline">\(X_i\)</span> (c’est le principe du Leave-one-out):</p>
<p><span class="math display">\[
\hat{G} = \frac{1}{n}\sum_{i=1}^n\hat {f}_{n,h}^{(-i)}(X_i)
\]</span>
avec :</p>
<p><span class="math display">\[
  \hat {f}_{n,h}^{(-i)}(x)= \frac{1}{n-1}\frac{1}{h}\sum_{j=1,j\ne i}^nK(\frac{x-X_j}{h})
\]</span></p>
<p>Montrons que <span class="math inline">\(\mathbb{E}(\hat{G})=\mathbb{E}[\int \hat{f}_{n,h}(x)f(x)dx]\)</span>.
Comme les <span class="math inline">\(X_i\)</span> sont i.i.d., d’une part nous avons :
<span class="math display">\[
\begin{aligned}
\mathbb{E}[\int \hat {f}_{n,h}(x)f(x)dx]&amp;= \mathbb{E}[\int \frac {1}{nh}\sum_{i=1}^nK(\frac {x-X_i}{h})f(x)dx]\\
&amp;=\frac{1}{h}\mathbb{E}[\int K(\frac {x-x_1}{h})f(x)dx] \\
&amp;=\frac{1}{h}\int f(x)\int K(\frac {x-X_1}{h})f(x_1)dx_1dx
\end{aligned}
\]</span>
D’autre part, nous avons :
<span class="math display">\[ 
\begin{aligned}
\mathbb{E}[\hat{G}]&amp;=\mathbb{E}[\frac{1}{n}\sum_{i=1}^n\hat{f}_{n,h}^{(-i)}(X_i)]
=\mathbb{E}[\hat{f}_{n,h}^{(-1)}(X_1)]\\
&amp;=\mathbb{E}[\frac{1}{(n-1)h}\sum_{j\ne 1}K(\frac{X_j-X_1}{h})]\\
&amp;=\mathbb{E}[\frac{1}{h}K(\frac{X-X_1}{h})]\\
&amp;=\frac{1}{h}\int f(x)\int K(\frac{x-x_1}{h})f(x_1)dx_1dx\\
&amp;=\mathbb{E}[\int \hat{f}_{n,h}(x)f(x)dx] 
\end{aligned}
\]</span></p>
<p>Donc, <span class="math inline">\(\hat{G}\)</span> est un estimateur sans biais de <span class="math inline">\(\int\hat{f}_{n,h}(x)f(x)dx\)</span>. Finalement, l’estimateur sans biais de <span class="math inline">\(R(\hat{f}_{n,h}, f)-\begin{Vmatrix}{f}\end{Vmatrix}_2^2\)</span> est donné par:</p>
<p><span class="math display">\[
\hat{R}(h)=\begin{Vmatrix}\hat{f}_{n,h}\end{Vmatrix}_2^2-\frac{2}{n(n-1)}\sum_{i=1}\sum_{j=1,j\ne i}\frac{1}{h}K(\frac{X_i-X_j}{h})
\]</span></p>
<p>On définit alors</p>
<p><span class="math display" id="eq:oracle">\[
\hat{h} = arg\ \underset{h\in H}{min}\hat{R}(h)
\]</span></p>
<p>Si ce minimum est atteint. On cherche une fenêtre parmi une grille finie de valeurs, grille qu’on a notée <span class="math inline">\(H\)</span> dans la formule ci-dessus.<br />
L’estimateur <span class="math inline">\(\hat{f}_{n,\hat{h}}\)</span> a de bonnes propriétés pratiques et de consistance.
La validation croisée est une méthode très générale mais nous l’utilisons ici pour le choix de la fenêtre <span class="math inline">\(h\)</span> optimale.</p>

</div>
<div id="méthode-de-goldenshluger-lepski" class="section level4">
<h4><span class="header-section-number">3.2.3.2</span> Méthode de Goldenshluger-Lepski</h4>
<p>La méthode de Goldenshluger-Lepski donne principalement des critères de sélection dans une famille d’estimateurs linéaires à noyau, afin d’obtenir un estimateur vérifiant une inégalité d’oracle.<br />
Avant de présenter ces critères de sélection, commençons d’abord par une introduction aux inégalités d’oracle dans l’estimation adaptative.</p>
<div id="inégalités-doracle" class="section level5">
<h5><span class="header-section-number">3.2.3.2.1</span> Inégalités d’oracle</h5>
References pour cette partie.\tag{3.1} et \tag{3.2} 
Supposons que la fonction estimée appartient à une classe fonctionnelle <span class="math inline">\(\mathcal{F}\)</span> et qu’on a un nombre d’observations n fixé.<br />
On a pour objectif de choisir, dans une famille d’estimateurs <span class="math inline">\(\mathbb{F} =\)</span>{<span class="math inline">\(\hat{f}_h ; h\in \mathcal{H}\)</span>} indexée par le paramètre <span class="math inline">\(h \in \mathcal{H}\)</span>, un estimateur <span class="math inline">\(\hat{f}_{h^*}\)</span> qui soit le meilleur possible.<br />
Cela revient à résoudre le problème de minimisation
<span class="math display">\[
h^*=arg~inf_{h \in \mathcal{H}} R(\hat{f}_h,f)
\]</span>
L’estimateur <span class="math inline">\(\hat{f}_{h^*}\)</span> n’est pas calculable en pratique puisqu’il dépend de la fonction inconnue <span class="math inline">\(f\)</span>. C’est aussi pourquoi il est souvent appelé oracle. Le but est donc de se servir de son risque pour trouver un estimateur qui fonctionne presque comme cet oracle. Pour cela nous utilisons l’échantillon des observations pour sélectionner un paramètre <span class="math inline">\(\hat{h} \in \mathcal{H}\)</span> tel que <span class="math inline">\(\hat{f}_{\hat{h}}\)</span> vérifie une égalité d’oracle
<span class="math display">\[
\mathcal{R}(\hat{f}_{\hat{h}},f) \leq C~inf_{h\in \mathcal{H}}~\mathcal{R}(\hat{f}_{h},f)+\delta,~~~~\forall~~f \in \mathcal{F}.
\]</span>
où <span class="math inline">\(C \geq 1\)</span> est une constante indépendante de n et de f, <span class="math inline">\(inf_{h\in \mathcal{H}}R(\hat{f}_h,f)\)</span> est le risque d’oracle et <span class="math inline">\(\delta\)</span> un terme résiduel indépendant de f, souvent négligeable devant le risque d’oracle.<br />


<p>La question qui se pose dans la suite est 
Soit <span class="math inline">\(\mathcal{F}\)</span> une collection d’estimateurs construits à partir des données et <span class="math inline">\(\hat{f}_h \rightarrow \mathcal{R}(\hat{f}_h,f)\)</span> un risque pour l’estimation de f, comment construire un estimateur <span class="math inline">\(\hat{f}_{\hat h}\)</span> tel que <span class="math inline">\(\mathcal{R}(\hat{f}_{\hat h},f) \approx~~inf_{h \in \mathcal{H}} \mathcal{R}(\hat{f}_h,f)\)</span> où <span class="math inline">\(\mathbb{E}[\mathcal{R}(\hat{f}_{\hat{h}},f)] \approx inf_{h \in \mathcal {H}} \mathbb{E}[\mathcal{R}(\hat{f}_h,f)]\)</span>?
C’est là que s’applique la méthode de Goldenshluger et Lepski. Il s’agit une des méthodes usuelles qui imite la décomposition biais-variance du risque de l’estimateur. 
Cette méthode se base sur l’observation. Elle consiste à choisir un estimateur dans une famille d’estimateurs linéaires <span class="math inline">\(\mathbb{F} =\)</span>{<span class="math inline">\(\hat{f}_h,h \in \mathcal{H}\)</span>}. Pour cela on doit imposer d’abord quelques suppositions. 
<em>Suppositions</em> 
-1) Le noyau K est lipschitzienne</p>
<p><span class="math display">\[
|K(x)~-~K(y)| \leq c|x-y|,~~~~\forall(x,y)\in \mathbb{R}.
\]</span>
OÙ |.| est la distance euclidienne.
-2)Il existe un réel <span class="math inline">\(k_{\infty}&lt;\infty\)</span> tel que <span class="math inline">\(||K||_{\infty} \leq k_{\infty}\)</span></p>
Passons ensuite au critère de sélection.
<em>Critère de sélection</em> 
Ce critère comme abordé au dessus se base sur la comparaisons des estimateurs deux à deux en faisant intervenir des estimateurs auxiliaires {<span class="math inline">\(\hat f_{h,\mu},h~~ et~~ \mu \in \mathcal{H}\)</span>} définies comme suit
<span class="math display">\[
\hat f_{h,\mu}(x)=\frac{1}{n}\sum^n_{i=1}[K_h * K_{\mu}](x-X_i),
\]</span>
où * est le produit de convolution sur <span class="math inline">\(\mathbb{R}\)</span>.
On définit aussi
<span class="math display">\[
\forall h \in \mathcal{H}~~\hat{\mathcal R}_h = sup_{\mu \in \mathcal{H}}[||\hat f_{h,\mu}-\hat f_\mu ||_s - m_s(h,\mu)]_+ + m^*_s(h),
\]</span>
où la fonction <span class="math inline">\(m_s\)</span> est appelé le majorant et <span class="math inline">\(m^*_s(h) = sup_{\mu \in \mathcal{H}}m_s(h,\mu) ~~ \forall h \in \mathcal{H}\)</span>


<p>De tout ce qui précède <span class="math inline">\(\hat h\)</span> est définie par</p>
<p><span class="math display">\[
\hat h = arg~inf_{h \in \mathcal{H}} \mathcal{\hat R}_h.
\]</span></p>
<p>Et le critère de comparaison</p>
<span class="math display">\[
\hat{\Delta}(h)= sup_{\mu \in \mathcal{H}}[||\hat f_{h,\mu}-\hat f_\mu||_s - m_s(h,\mu)]_+,~~~~\forall h,\mu \in \mathcal H,~~\forall \hat f \in \mathbb F.
\]</span>

<p>Donc si l’inégalité suivante est vérifiée</p>
<p><span class="math display">\[
sup_{h \in \mathcal{H}}(\mathbb{E}_f~sup_{\mu \in \mathcal H}[||\xi_{h,\mu}-\xi_\mu || - m_s(h,\mu)]_+^2)^{\frac{1}{2}} \leq \delta,~~\forall f \in \mathbb{F}, ~~\forall h,\mu \in \mathcal H.
\]</span>
Et si il existe <span class="math inline">\(\hat h \in \mathcal{H}\)</span> mesurable par rapport à l’observation et vérifiant</p>
<p><span class="math display" id="eq:cours-est" id="eq:est-ad" id="eq:oracle">\[
\hat{\Delta}(\hat h)+sup_{\mu \in \mathcal H}m_s(\hat h, \mu) \leq inf_{h \in \mathcal H}(\hat \Delta(h) +sup_{\mu \in \mathcal H}m_s(h,\mu)).
\]</span>
Alors l’estimation sectionnée est <span class="math inline">\(\hat f_{\hat h}\)</span>.</p>

</div>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="méthodes-non-paramétriques.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="applications.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Book especes.pdf", "Book especes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
