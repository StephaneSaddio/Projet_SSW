[["intro.html", "Rapport du projet de M1 Combien de temps pour faire une espèce ? Chapter 1 Introduction 1.1 Motivation 1.2 Problématique", " Rapport du projet de M1 Combien de temps pour faire une espèce ? Wiam Chaoui Sophie Manuel Stéphane Sadio 2021 Chapter 1 Introduction 1.1 Motivation La classification du vivant est depuis longtemps un vrai casse-tête pour les biologistes, surtout en ce qui concerne la notion despèce. De fait, il existe plusieurs définitions du mot espèce, ce qui rend encore plus compliqué un consensus. Cest pour cela que dans la suite nous ne nous étendrons pas sur cette notion et nous ne nous concentrerons que sur des espèces prédéfinies. Les arbres phylogénétiques sont des outils permettant de représenter graphiquement certaines données de classification. En effet, ils présentent les relations de parenté entre espèces. On retrouve dessous différentes espèces actuelles, mais aussi leurs ancêtres communs (les branchements évolutifs qui correspondent à lapparition dune nouvelle homologie), ou encore la durée avant lapparition dune nouvelle espèce qui est donnée par la longueur des branches. Dans la suite, nous nous intéresserons aux branchements évolutifs. Supposons quun branchement évolutif apparaît après une durée aléatoire dune loi fixée \\(\\mu\\) indépendamment du passé et du futur évolutif des espèces. Quelle est cette loi \\(\\mu\\)? Sa variance ? Sa moyenne ? On observe des branchements successifs qui composent larbre phylogénétique et à partir de ces données quantitatives observées, on veut estimer la fonction de densité \\(f\\) qui donne la probabilité quun nouveau branchement évolutif apparaisse après un certain temps. Formellement on a le modèle densité suivant, soient les vecteurs aléatoires \\(X_1,\\dots,X_n\\) tel que \\(n \\in \\mathbb{N^*}\\) à valeur dans \\(\\mathbb{R}\\), indépendants et identiquement distribués de longueurs de branche observées qui ont pour une fonction de densité \\(f\\) par rapport à la mesure de Lebesgue sur \\(\\mathbb{R}\\) supposée inconnue. Notre objectif est destimer cette fonction densité f sur laquelle on fait le moins dhypothèses possibles. On fera seulement les hypothèses dexistence, de continuité et de positivité de la fonction,en servant une observation \\((X_1,...X_n)\\), ce qui nous mène en statistique non-paramétrique, où le paramètre cherché est une densité de probabilité qui appartient à un espace fonctionnel infini, doù la problématique de notre sujet. 1.2 Problématique Comment estimer la loi de densité de la création dune nouvelle espèce avec une méthode destimation non-paramétrique ? Pour commencer, en se basant sur quelques définitions nous présenterons les méthodes destimations non-paramétriques et en introduisant quelques types destimateurs. Par la suite, nous approfondirons les estimateurs de densité à noyau en menant une discussion sur leurs critères dévaluation. Ainsi nous consacrerons un chapitre pour présenter des méthodes adaptatives. Enfin, pour répondre à la problématique, nous implémenterons un estimateur à noyau adaptatif, de type Goldenshluger-Lepski puis lutiliser sur des données darbres phylogénétiques. "],["méthodes-non-paramétriques.html", "Chapter 2 Méthodes non-paramétriques 2.1 Introduction 2.2 Estimation par histogramme 2.3 Estimateurs par projection : 2.4 Estimateurs à noyau de densité", " Chapter 2 Méthodes non-paramétriques En statistique, on parle destimation quand on cherche à trouver certains paramètres inconnus caractérisant une distribution à partir dun échantillon de données observées, en se basant sur différentes méthodes. On se tourne vers lestimation non-paramétrique lorsque lon traite des paramètres à dimension infinie. Dans notre cas, ce paramètre est la fonction densité appartenant à un espace fonctionnel. Nous présenterons dans la suite une courte introduction à lestimation non-paramétrique et nous introduirons les deux classes principales de lestimation fonctionnelle (lestimation par projection et lestimation à noyau). 2.1 Introduction Lestimation non-paramétrique vise à résoudre des problèmes destimation dans le cadre statistique où le modèle auquel on sintéresse nest pas décrit par un nombre fini de paramètres et dont chacun de ces paramètres ne permet pas de décrire la structure générale de la distribution des variables aléatoires.Cela signifie quon utilise des modèles statistiques à dimension infinie. Dans le cadre de notre problématique on sintéresse à lestimation de densité. Un des principes de base de lestimation de la densité selon une méthode destimation non-paramétrique est le suivant Soit un échantillon dobservations \\(X=\\{X_1, \\dots,X_n\\}\\) de variables aléatoires i.i.d admettant une densité \\(f= F&#39;\\). Supposons que \\(f \\in \\mathcal F\\) où \\(\\mathcal{F}\\) est un espace fonctionnel. On cherche à estimer la fonction de densité inconnue \\(f\\) à partir de ces observations. On notera \\(\\hat f\\) lestimateur de f. On se trouve donc avec le modèle suivant \\(\\{\\mathbb P=\\mathbb P_f,~f \\in \\mathcal F\\}\\) où \\(\\mathbb P_f\\) est la mesure probabilité de la densité \\(f\\). Lestimation ici concerne donc la fonction elle même plutôt que les paramètres, ce qui explique le nom destimation non-paramétrique. 2.2 Estimation par histogramme Une des premières approches possibles destimations non-paramétriques de la fonction de densité est lestimation par lhistogramme. Cest une méthode qui consiste à obtenir un graphique de type histogramme pour la répartition des observations et à considèrer cet histogramme comme une approximation de la fonction densité f.\\tag{2.1} (ajout formule graphique) Nous traiterons dans la suite deux grandes familles de méthodes linéaires pour estimer une fonction densité : - Lestimation par projection, - Lestimation par noyau. 2.3 Estimateurs par projection : Dans la suite on procédera à la méthode la plus fréquemment utilisée pour lestimation dune densité : Lestimation à noyau. (Argument pour le choix de la méthode à voir) 2.4 Estimateurs à noyau de densité Notre but est destimer la densité \\(f\\). Pour cela, on sappuiera sur un échantillon \\(i.i.d.\\) \\(X= (X_1,...,X_n)\\) où chacune des variables \\(X_i\\) admet la densité \\(f\\) (par rapport à la mesure de Lebesgue). Pour estimer une densité on peut utiliser une méthode à noyau. Les méthodes à noyau sont des méthodes non-paramétriques qui permettent de proposer une estimation de la densité plus lisse que celle obtenue par un histogramme. 2.4.1 Comment construit-on un estimateur à noyau ? Lidée pour la construction de cet estimateur est dutiliser lapproximation suivante , valable lorsque h est petit : \\[ f(x) = F&#39;(x)\\approx \\frac{F(x+h)-F(x-h)}{2h} \\] Pour estimer la densité \\(f\\) on peut passer par un estimateur \\(\\hat F_n\\) de la fonction de répartition \\(F\\). \\(\\hat F_n\\) est la fonction de répartition empirique ( \\(\\hat F_n(x)= \\frac1n \\sum\\limits_{i=1}^n\\frac{1}{2h} \\mathds1_{X_i \\in ]x-h, x+h[}\\) ). \\[ \\hat f_n(x)= \\frac{\\hat F_n(x+h)-\\hat F_n(x-h)}{2h} = \\frac 1n \\sum\\limits_{i=1}^n \\frac1{2h} \\mathds1_{X_i \\in ]x-h;x+h]} \\] Notons \\(\\hat f(x)\\) lestimateur à noyau de la densité \\(f\\), alors celui-ci sécrit : \\[ \\hat f_n(x) = \\frac1{nh} \\sum\\limits_{i=1}^n K\\left(\\frac{X_i-x}h\\right) \\] où \\(h\\) est la fenêtre (ou paramètre de lissage), \\(n\\) le nombre dobservations, et \\(K\\) le noyau. Cette formule nest valable que si \\(h\\) est petit et positif. ici \\(K(u)= \\frac12 \\mathds{1}_{u \\in ]-1;1]}\\), il sagit du noyau de Rosenblatt, mais il existe dautres noyaux. 2.4.2 Quest un noyau ? Exemples de noyau : Noyau de Rosenblatt, ou rectangulaire : \\(K(u)= \\frac12 \\mathds{1}_{u\\in]-1;1]}\\) Noyau Gaussien : \\(K(u) =\\frac1{\\sqrt{2\\pi}}exp(-\\frac{u^2}2)\\) Noyau dEpanechnikov : \\(K(u) =\\frac34(1-u^2)\\mathds{1}_{[-1,1]}(u)\\) Noyau triangulaire : \\(K(u) = (1-\\mid u \\mid)\\mathds{1}_{[-1,1]}(u)\\) Noyau Biweight : \\(K(u) = \\frac{15}{16}(1-u^2)^2\\mathds{1}_{[-1,1]}(u)\\) Les propriétés du noyau (continuité, différentiabilité) se transmettent à lestimateur \\(\\hat f_n\\). "],["estimateur-de-densité-à-noyau.html", "Chapter 3 Estimateur de densité à noyau 3.1 Evaluer un estimateur 3.2 Méthodes adaptatives", " Chapter 3 Estimateur de densité à noyau 3.1 Evaluer un estimateur Avant de commencer cette partie on va dabord introduire quelques notions et définitions Pour évaluer un estimateur \\(\\hat{f}\\) on définit son risque associé. 3.1.1 Risque quadratique des estimateurs à noyau sur les classe des espaces de Hölder Nous nous intéressons au risque quadratique de \\(\\hat{f}_n\\), définit par : étant donné \\(x_0 \\in \\mathbb{R}\\) \\[ R(\\hat {f}_n, f) = \\mathbb{E}[|\\hat {f}_n(x_0) - f(x_0)|^2] \\] Rappelons la décomposition biais-variance du risque quadratique : \\[ \\mathbb{E}[|\\hat {f}_n(x_0) - f(x_0)|^2] = (\\mathbb{E}[\\hat {f}_n(x_0)] - f(x_0))^2 + \\mathbb{V}[\\hat {f}_n(x_0)] \\] 3.1.1.1 Majoration du biais et de la variance Dans cette section, nous allons nous intéresser au compromis biais-variance afin de minimiser le risque quadratique. Nous introduirons après quelques définitions deux propositions qui montrent que sous certaines hypothèses, on peut majorer le biais ainsi que la variance. \\begin{exem} (Simulation numérique) Nous estimons la fonction densité dune somme de deux variables gaussiennes ci-contre avec la méthode à noyau avec différentes fenêtres. \\[ f(x)=\\frac{1}{2}\\frac{1}{\\sqrt{2\\pi}}(exp(-\\frac{(x-2)^2}{2})+exp(-\\frac{(x-6)^2}{2})) \\] On va en fait utiliser ggplot pour représenter lestimateur à noyau. La fonction qui permet de dessiner lestimateur à noyau est geom_density. Le paramètre représentant le fenêtre h rappelle bw (comme bandwidth en Anglais). \\end{exem} 3.2 Méthodes adaptatives On introduit précédemment la notion de lestimation de la densité qui dépend dun paramètre de lissage \\(h\\). Soit \\((\\hat{f_h})_{h\\in \\mathcal H}\\) une famille des estimateurs de la fonction densité \\(f\\) que que lon cherchons cherche à estimer. Une question simpose : comment peut-on construire un estimateur à risque optimal à partir de cette famille en tenant en considération les observations ? Dans la théorie adaptative f est toujours supposée appartenir à une classe fonctionnelle. Cette classe nest pas connu à priori mais supposé appartenir à une famille de classes fonctionnelles{\\(\\mathcal{F_{\\alpha}},\\alpha \\in\\mathcal{A}\\)} où \\(\\mathcal{A}\\) est un ensemble des paramètres de nuisance. ( ref : Sur lestimation adaptative dune densité multivariée sous lhypothèse de la structure dindépendance-Approche minimax adaptative). Dans cette partie, afin de répondre à la question posée auparavant, nous allons discuter du choix du noyau en premier lieu. Ensuite, nous introduirons deux méthodes pour le choix du paramètre de lissage \\(h\\). 3.2.1 Choix du noyau Avant de présenter le critère de choix du noyau nous allons introduire quelques outils mathématiques qui simplifient lécriture du critère. Tout dabord, nous avons besoin du risque quadratique \\(\\mathcal R\\) aussi appelé lerreur quadratique moyenne (Mean Squared Error en anglais). Dans cette partie nous allons noter \\(MSE\\) le risque quadratique pour des questions pratiques. \\[ \\begin{aligned} MSE &amp;= \\mathcal R = \\mathbb E \\left[ \\{ \\hat f_n(x)-f(x) \\} \\right] \\\\ &amp;=\\mathbb V \\left[ \\hat f_n(x) \\right] + Biais^2 \\left[ \\hat f_n(x) \\right] \\\\ &amp;= MSE(x ; n, h, K, f). \\end{aligned} \\] comme nous lavons montré précédemment. @ref{#holder} Lapproximation de lerreur quadratique (Average of Mean Squared Error en anglais) est donnée par léquation suivante : \\[ AMSE(x)= \\frac1{nh} f(x) \\int_{\\mathbb R}K(t)^2 dt + \\left(\\frac12h^2f&#39;&#39;(x)\\int_{\\mathbb R} t^2K(t) dt \\right)^2. \\] Elle a été calculée à partir de la variance approchée et du biais approché. Lerreur quadratique moyenne intégrée (Mean Intregrated Squared Error en anglais) est une mesure théorique communément utilisée pour évaluer la différence entre \\(f\\) et \\(\\hat f_n\\). Pour lévaluer on utilise lerreur quadratique moyenne quon intègre sur le support \\(\\mathbb R\\) de lestimateur. \\[ \\begin{aligned} MISE(n,h,K,f) &amp;= \\int_{\\mathbb R}MSE(x;n,h,K,f)dx\\\\ &amp;= \\int_{\\mathbb R} \\mathbb V\\left[ \\hat f_n(x) \\right] dx+ \\int_{\\mathbb R} Biais^2 \\left[ \\hat f_n(x) \\right]dx \\end{aligned} \\] De la même façon que nous lavons fait avec lerreur quadratique moyenne, nous allons calculer lexpression approchée de lerreur quadratique moyenne (Average of Mean Integrated Squared Error en anglais). \\[ \\begin{aligned} AMISE(x)&amp;= \\frac1{nh} \\int_{\\mathbb R}f(x)dx \\int_{\\mathbb R}K(t)^2 dt + \\frac14h^4 \\int_{\\mathbb R}f&#39;&#39;(x)dx ~~\\left(\\int_{\\mathbb R} t^2K(t) dt \\right)^2\\\\ &amp;= \\frac1{nh} \\int_{\\mathbb R}K(t)^2 dt + \\frac14h^4 \\int_{\\mathbb R}f&#39;&#39;(x)dx ~~\\left(\\int_{\\mathbb R} t^2K(t) dt \\right)^2\\\\ &amp;= \\frac1{nh} \\int_{\\mathbb R}K(t)^2 dt + \\frac14h^4 ~ \\mathbb V (K)^2 \\int_{\\mathbb R}f&#39;&#39;(x)dx \\end{aligned} \\] à présent que nous avons défini les outils nécessaires au choix du noyau, nous allons pouvoir présenter un critère de choix pour les noyaux continus symétriques. Afin de mesurer lefficacité des noyaux, nous utilisons une mesure qui calcule le rapport du critère \\(AMISE\\) de deux noyaux. \\[ eff(K_1,K_2) = \\frac{AMISE(K_1)}{AMISE(K_2)}. \\] Supposons que \\(K_1\\) est le noyau dEpanechnikov, il est souvent utilisé comme référence par rapport aux autres noyaux continus. Après quelques calculs, on obtient que lefficacité dun noyau K par rapport à celui dEpanechnikov est donnée par \\[ eff(K) = \\frac{3}{5\\times \\int_{\\mathbb R}K(t)^2 dt\\sqrt{5 \\times\\int_{\\mathbb R}t^2K(t) dt} } \\leq1. \\] Voici un tableau récapitulatif de lefficacité de plusieurs noyaux continus symétriques. (#tab:ker_tab)Efficacité des noyaux continus symétriques Noyau Efficacité Epanechnikov 1.000 Bigweight 0.994 Triangular 0.986 Gaussien 0.951 Rectangulaire 0.930 3.2.2 Comment choisir les paramètres de la méthode ? Dans la méthode destimation à noyau le choix du noyau nest pas le plus important, le vrai enjeu de cette méthode est le choix de la fenêtre \\(h\\) (bandwidth). En effet, la fenêtre détermine linfluence des données dans lestimation. Si \\(h\\) est petit, leffet local est important donc on aura beaucoup de bruit. Si \\(h\\) est grand, on aura une estimation plus douce, plus lisse. Nous pouvons constater linfluence du paramètre \\(h\\) sur lexemple suivant : Nous avons simulé 500 variables suivant une loi de Weibull de paramètres (\\(\\alpha = 1.7\\), \\(\\lambda=2\\)) représentées dans lhistogramme. La courbe en rouge est la fonction de densité de la loi de Weibull et la bleue est lestimation avec la méthode des noyaux sur les variables simulées. La fenêtre \\(h\\) du second graphique est calculée automatiquement par la fonction densityde R. 3.2.3 Choix de la fenêtre Lestimation de densité nécessite de faire le choix de la taille de la fenêtre quon note \\(h\\).En statistique non-paramétrique, ils existent plusieurs méthodes et critères de qualité pour le choix de la fenêtre. On présente dans la suite deux méthodes : * Méthode de validation croisée * Méthode de Goldenshluger-Lepski. 3.2.3.1 Choix de la fenêtre \\(h\\) par validation croisée Le choix de la fenêtre dans la section précédente est critiquable: comme on la mentionné, il dépend de la régularité de la fonction \\(f\\) qui est inconnue dans notre cas. On peut donc essayer destimer cette fenêtre idéale par un estimateur \\(\\hat{h}\\). De façon à souligner la dépendance à la fonction, on va noter \\(\\hat{f}_{n,h}\\) lestimateur associé à un choix de fenêtre \\(h\\). Lestimateur final sera \\(\\hat{f}_{n,\\hat{h}}\\), une fois le choix de \\(\\hat{h}\\) fait. On cherche à minimiser en \\(h\\) le risque quadratique pour la distance \\(L_2\\) : \\[ \\begin{aligned} R(\\hat {f}_{n,h})&amp;=\\mathbb{E}[\\begin{Vmatrix}\\hat {f}_{n,h}-f\\end{Vmatrix}_2^2]\\\\ &amp;= \\mathbb{E}[\\begin{Vmatrix}\\hat {f}_{n,h}\\end{Vmatrix}_2^2] -2~\\mathbb{E}[\\int \\hat {f}_{n,h}(x)f(x)dx] +\\begin{Vmatrix}f\\end{Vmatrix}_2^2 \\end{aligned} \\] Or la fonction \\(f\\) étant inconnue, ce risque nest pas calculable à partir des données. On cherche donc à estimer ce risque en utilisant uniquement les données. Remarquons que minimiser en \\(h\\) la quantité \\(R(\\hat {f}_{n,h}, f)\\) est équivalent à minimiser en \\(h\\) la quantité \\(R(\\hat {f}_{n,h}, f)-\\begin{Vmatrix}f\\end{Vmatrix}_2^2\\). On va en fait remplacer la minimisation de la quantité inconnue \\(R(\\hat {f}_{n,h}, f)-\\begin{Vmatrix}f\\end{Vmatrix}_2^2\\) par la minimisation dun estimateur \\(\\hat {R}(h)\\) de cette quantité. Plus précisément on va chercher un estimateur sans biais de cette expression: \\[ \\mathbb{E}[\\begin{Vmatrix}\\hat {f}_{n,h}\\end{Vmatrix}_2^2] -2~\\mathbb{E}[\\int \\hat {f}_{n,h}(x)f(x)dx] \\] Le premier terme admet \\(\\begin{Vmatrix}\\hat {f}_{n,h}\\end{Vmatrix}_2^2\\) comme estimateur trivial (daprès la propriété des estimateurs sans biais : \\(\\mathbb{E}[\\hat {\\beta}]=\\beta\\)). Il reste à trouver un estimateur sans biais du second terme. Pour cela, nous admettons par construction lestimateur sans biais \\(\\hat {G}\\) définit en tout points sauf en \\(X_i\\) (cest le principe du Leave-one-out): \\[ \\hat{G} = \\frac{1}{n}\\sum_{i=1}^n\\hat {f}_{n,h}^{(-i)}(X_i) \\] avec : \\[ \\hat {f}_{n,h}^{(-i)}(x)= \\frac{1}{n-1}\\frac{1}{h}\\sum_{j=1,j\\ne i}^nK(\\frac{x-X_j}{h}) \\] Montrons que \\(\\mathbb{E}(\\hat{G})=\\mathbb{E}[\\int \\hat{f}_{n,h}(x)f(x)dx]\\). Comme les \\(X_i\\) sont i.i.d., dune part nous avons : \\[ \\begin{aligned} \\mathbb{E}[\\int \\hat {f}_{n,h}(x)f(x)dx]&amp;= \\mathbb{E}[\\int \\frac {1}{nh}\\sum_{i=1}^nK(\\frac {x-X_i}{h})f(x)dx]\\\\ &amp;=\\frac{1}{h}\\mathbb{E}[\\int K(\\frac {x-x_1}{h})f(x)dx] \\\\ &amp;=\\frac{1}{h}\\int f(x)\\int K(\\frac {x-X_1}{h})f(x_1)dx_1dx \\end{aligned} \\] Dautre part, nous avons : \\[ \\begin{aligned} \\mathbb{E}[\\hat{G}]&amp;=\\mathbb{E}[\\frac{1}{n}\\sum_{i=1}^n\\hat{f}_{n,h}^{(-i)}(X_i)] =\\mathbb{E}[\\hat{f}_{n,h}^{(-1)}(X_1)]\\\\ &amp;=\\mathbb{E}[\\frac{1}{(n-1)h}\\sum_{j\\ne 1}K(\\frac{X_j-X_1}{h})]\\\\ &amp;=\\mathbb{E}[\\frac{1}{h}K(\\frac{X-X_1}{h})]\\\\ &amp;=\\frac{1}{h}\\int f(x)\\int K(\\frac{x-x_1}{h})f(x_1)dx_1dx\\\\ &amp;=\\mathbb{E}[\\int \\hat{f}_{n,h}(x)f(x)dx] \\end{aligned} \\] Donc, \\(\\hat{G}\\) est un estimateur sans biais de \\(\\int\\hat{f}_{n,h}(x)f(x)dx\\). Finalement, lestimateur sans biais de \\(R(\\hat{f}_{n,h}, f)-\\begin{Vmatrix}{f}\\end{Vmatrix}_2^2\\) est donné par: \\[ \\hat{R}(h)=\\begin{Vmatrix}\\hat{f}_{n,h}\\end{Vmatrix}_2^2-\\frac{2}{n(n-1)}\\sum_{i=1}\\sum_{j=1,j\\ne i}\\frac{1}{h}K(\\frac{X_i-X_j}{h}) \\] On définit alors \\[ \\hat{h} = arg\\ \\underset{h\\in H}{min}\\hat{R}(h) \\] Si ce minimum est atteint. On cherche une fenêtre parmi une grille finie de valeurs, grille quon a notée \\(H\\) dans la formule ci-dessus. Lestimateur \\(\\hat{f}_{n,\\hat{h}}\\) a de bonnes propriétés pratiques et de consistance. La validation croisée est une méthode très générale mais nous lutilisons ici pour le choix de la fenêtre \\(h\\) optimale. 3.2.3.2 Méthode de Goldenshluger-Lepski La méthode de Goldenshluger-Lepski donne principalement des critères de sélection dans une famille destimateurs linéaires à noyau, afin dobtenir un estimateur vérifiant une inégalité doracle. Avant de présenter ces critères de sélection, commençons dabord par une introduction aux inégalités doracle dans lestimation adaptative. 3.2.3.2.1 Inégalités doracle References pour cette partie.\\tag{3.1} et \\tag{3.2} Supposons que la fonction estimée appartient à une classe fonctionnelle \\(\\mathcal{F}\\) et quon a un nombre dobservations n fixé. On a pour objectif de choisir, dans une famille destimateurs \\(\\mathbb{F} =\\){\\(\\hat{f}_h ; h\\in \\mathcal{H}\\)} indexée par le paramètre \\(h \\in \\mathcal{H}\\), un estimateur \\(\\hat{f}_{h^*}\\) qui soit le meilleur possible. Cela revient à résoudre le problème de minimisation \\[ h^*=arg~inf_{h \\in \\mathcal{H}} R(\\hat{f}_h,f) \\] Lestimateur \\(\\hat{f}_{h^*}\\) nest pas calculable en pratique puisquil dépend de la fonction inconnue \\(f\\). Cest aussi pourquoi il est souvent appelé oracle. Le but est donc de se servir de son risque pour trouver un estimateur qui fonctionne presque comme cet oracle. Pour cela nous utilisons léchantillon des observations pour sélectionner un paramètre \\(\\hat{h} \\in \\mathcal{H}\\) tel que \\(\\hat{f}_{\\hat{h}}\\) vérifie une égalité doracle \\[ \\mathcal{R}(\\hat{f}_{\\hat{h}},f) \\leq C~inf_{h\\in \\mathcal{H}}~\\mathcal{R}(\\hat{f}_{h},f)+\\delta,~~~~\\forall~~f \\in \\mathcal{F}. \\] où \\(C \\geq 1\\) est une constante indépendante de n et de f, \\(inf_{h\\in \\mathcal{H}}R(\\hat{f}_h,f)\\) est le risque doracle et \\(\\delta\\) un terme résiduel indépendant de f, souvent négligeable devant le risque doracle. La question qui se pose dans la suite est Soit \\(\\mathcal{F}\\) une collection destimateurs construits à partir des données et \\(\\hat{f}_h \\rightarrow \\mathcal{R}(\\hat{f}_h,f)\\) un risque pour lestimation de f, comment construire un estimateur \\(\\hat{f}_{\\hat h}\\) tel que \\(\\mathcal{R}(\\hat{f}_{\\hat h},f) \\approx~~inf_{h \\in \\mathcal{H}} \\mathcal{R}(\\hat{f}_h,f)\\) où \\(\\mathbb{E}[\\mathcal{R}(\\hat{f}_{\\hat{h}},f)] \\approx inf_{h \\in \\mathcal {H}} \\mathbb{E}[\\mathcal{R}(\\hat{f}_h,f)]\\)? Cest là que sapplique la méthode de Goldenshluger et Lepski. Il sagit une des méthodes usuelles qui imite la décomposition biais-variance du risque de lestimateur. Cette méthode se base sur lobservation. Elle consiste à choisir un estimateur dans une famille destimateurs linéaires \\(\\mathbb{F} =\\){\\(\\hat{f}_h,h \\in \\mathcal{H}\\)}. Pour cela on doit imposer dabord quelques suppositions. Suppositions -1) Le noyau K est lipschitzienne \\[ |K(x)~-~K(y)| \\leq c|x-y|,~~~~\\forall(x,y)\\in \\mathbb{R}. \\] OÙ |.| est la distance euclidienne. -2)Il existe un réel \\(k_{\\infty}&lt;\\infty\\) tel que \\(||K||_{\\infty} \\leq k_{\\infty}\\) Passons ensuite au critère de sélection. Critère de sélection Ce critère comme abordé au dessus se base sur la comparaisons des estimateurs deux à deux en faisant intervenir des estimateurs auxiliaires {\\(\\hat f_{h,\\mu},h~~ et~~ \\mu \\in \\mathcal{H}\\)} définies comme suit \\[ \\hat f_{h,\\mu}(x)=\\frac{1}{n}\\sum^n_{i=1}[K_h * K_{\\mu}](x-X_i), \\] où * est le produit de convolution sur \\(\\mathbb{R}\\). On définit aussi \\[ \\forall h \\in \\mathcal{H}~~\\hat{\\mathcal R}_h = sup_{\\mu \\in \\mathcal{H}}[||\\hat f_{h,\\mu}-\\hat f_\\mu ||_s - m_s(h,\\mu)]_+ + m^*_s(h), \\] où la fonction \\(m_s\\) est appelé le majorant et \\(m^*_s(h) = sup_{\\mu \\in \\mathcal{H}}m_s(h,\\mu) ~~ \\forall h \\in \\mathcal{H}\\) De tout ce qui précède \\(\\hat h\\) est définie par \\[ \\hat h = arg~inf_{h \\in \\mathcal{H}} \\mathcal{\\hat R}_h. \\] Et le critère de comparaison \\[ \\hat{\\Delta}(h)= sup_{\\mu \\in \\mathcal{H}}[||\\hat f_{h,\\mu}-\\hat f_\\mu||_s - m_s(h,\\mu)]_+,~~~~\\forall h,\\mu \\in \\mathcal H,~~\\forall \\hat f \\in \\mathbb F. \\] Donc si linégalité suivante est vérifiée \\[ sup_{h \\in \\mathcal{H}}(\\mathbb{E}_f~sup_{\\mu \\in \\mathcal H}[||\\xi_{h,\\mu}-\\xi_\\mu || - m_s(h,\\mu)]_+^2)^{\\frac{1}{2}} \\leq \\delta,~~\\forall f \\in \\mathbb{F}, ~~\\forall h,\\mu \\in \\mathcal H. \\] Et si il existe \\(\\hat h \\in \\mathcal{H}\\) mesurable par rapport à lobservation et vérifiant \\[ \\hat{\\Delta}(\\hat h)+sup_{\\mu \\in \\mathcal H}m_s(\\hat h, \\mu) \\leq inf_{h \\in \\mathcal H}(\\hat \\Delta(h) +sup_{\\mu \\in \\mathcal H}m_s(h,\\mu)). \\] Alors lestimation sectionnée est \\(\\hat f_{\\hat h}\\). "],["applications.html", "Chapter 4 Applications 4.1 Fonction dens 4.2 Applications aux données du vivant", " Chapter 4 Applications Rappelons que nous cherchons à estimer la fonction de densité \\(f\\) de la durée avant la création dune nouvelle espèce. Dans ce cadre nous allons faire nos estimations à noyau de la densité sur R en appliquant la théorie que nous avons vue jusque là. 4.1 Fonction dens Dans cette partie nous présenterons la fonction dens que nous avons créée en voulant reproduire ce que fait la fonction density de R. Voici son code : Insérer le script de fonc_dens.R 4.2 Applications aux données du vivant Maintenant que nous avons notre fonction dens nous allons pouvoir la comparer à la fonction density de R. Pour les comparer nous allons en profiter pour en même temps les appliquer aux données quon veut étudier. Insérer test noyau.R Estimer loi espérance variance. "],["conclusion.html", "Chapter 5 Conclusion", " Chapter 5 Conclusion "],["references.html", "Chapter 6 References", " Chapter 6 References \\tag{3.1} Lucie Montuelle. Inégalités doracle et mélanges. Statistiques [math.ST]. Université Paris-Sud, 2014. Français. \\tag{3.2} Gilles Rebelles. Sur lestimation adaptative dune densité multivariée sous lhypothèse de la structure dindépendance. \\tag{2.1} Cours-Estimation de densité.https://cedric.cnam.fr/vertigo/Cours/ml/coursEstimationDensite.html "],["references-1.html", "References", " References "]]
