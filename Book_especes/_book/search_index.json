[["intro.html", "Rapport du projet de M1 Combien de temps pour faire une espèce ? Chapter 1 Introduction 1.1 Motivation 1.2 Problématique", " Rapport du projet de M1 Combien de temps pour faire une espèce ? Wiam Chaoui Sophie Manuel Stéphane Sadio 2021 Chapter 1 Introduction 1.1 Motivation La classification du vivant est depuis longtemps un vrai casse-tête pour les biologistes, surtout en ce qui concerne la notion despèce. De fait, il existe plusieurs définitions du mot espèce, ce qui rend encore plus compliqué un concensus. Cest pour cela que dans la suite nous ne nous étendrons pas sur cette notion et on se concentrera que sur des espèces prédéfinies. Les arbres phylogénétiques sont des outils permettant de représenter graphiquement certaines données de classification. En effet, ils présentent les relations de parenté entre espèces. On retrouve dessous différentes espèces actuelles, mais aussi leurs ancêtres communs (les branchements évolutifs qui correspondent à lapparition dune nouvelle homologie), ou encore la durée avant lapparition dune nouvelle espèce qui est donnée par la longeur des branches. Dans la suite, on sintéresse aux branchements évolutifs. On suppose quun branchement évolutif apparaît après une durée aléatoire dune loi fixée \\(\\mu\\) indépendamment du passé et du futur évolutif des espèces. Quelle est cette loi \\(\\mu\\)? Sa variance ? Sa moyenne ? On observe des branchements successifs qui composent larbre phylogénétique et à partir de ces données quantitatives observées, on veut estimer la fonction de densité \\(f\\) qui donne la probabilité quun nouveau branchement évolutif apparaisse après un certain temps. Formellement, on a un échantillon \\(X\\)={\\(X_1,\\dots,X_n\\)} de longueurs de branche observées qui ont pour une fonction de densité \\(f \\in \\mathcal F\\) où \\(\\mathcal F\\) est un espace fonctionnel. On cherche à estimer cette fonction densité \\(f\\) sur laquelle on fait le moins dhypothèses possibles. On fera seulement les hypothèses dexistence, de continuité et de positivité de la fonction \\(f\\). Doù le choix du modèle suivant \\(\\{P=P_f,~f\\in \\mathcal F\\}\\) qui revient à faire une estimation non-paramétrique de la densité. Ce qui nous mène à la problématique de notre sujet. 1.2 Problématique Comment estimer la loi de densité de la création dune nouvelle espèce avec une méthode destimation non-paramétrique ? Pour commencer, nous introduirons les méthodes destimations non-paramétriques, en donnant quelques définitions et en présentant quelques types destimateurs. Ensuite, nous allons approfondir sur les estimateurs de densité à noyau en parlant de leur évaluation et des méthodes adaptatives. Enfin, pour répondre à la problématique, on cherchera à implémenter un estimateur à noyau adaptatif, de type Goldenshluger-Lepski puis lutiliser sur des données darbres phylogénétiques. "],["méthodes-non-paramétriques.html", "Chapter 2 Méthodes non-paramétriques 2.1 Définitions 2.2 Estimateurs par projection : 2.3 Estimateurs à noyau de densité", " Chapter 2 Méthodes non-paramétriques En statistique, on parle destimation quand on cherche à trouver certains paramètres inconnus caractérisant une distribution à partir dun échantillon de données observées en se basant sur différentes méthodes. On se tourne vers lestimation non-paramétrique lorsquon traite des paramètres à dimension infini. Ce qui est bien notre cas, comme on cherche à estimer une fonction densité qui appartient à un espace fonctionnel. On présente dans la suite une courte introduction à lestimation non paramétrique. On introduira ensuite les deux classes principales de lestimation fonctionnelle (lestimation par projection et lestimation à noyau) afin de discuter de ces deux classes et expliquer pourquoi on fait le choix de lestimation à noyau. 2.1 Définitions Dans le cadre de notre problématique on sintéresse a lestimation de densité. Un des principes de base de lestimation de la densité selon une méthode destimation non-paramétrique est le suivant Lestimation ici concerne donc la fonction elle même plutôt que les paramètres, ce qui explique le nom destimation non-paramétrique. Une des premières estimations non-paramétriques de la fonction de densité qui est possible est lhistogramme. (ajout formule graphique) Lhistogramme fait partie de la famille des estimations à noyau quon détaillera plus tard. Nous traiterons deux grandes familles de méthodes pour estimer une fonction densité : - Lestimation par projection et - Lestimation par noyau. 2.2 Estimateurs par projection : Dans la suite on procèdera à la méthode la plus fréquemment utilisée pour lestimation dune densité : Lestimation à noyau. (Argument pour le choix de la méthode à voir) 2.3 Estimateurs à noyau de densité Notre but est destimer la densité \\(f\\). Pour cela, on sappuiera sur un échantillon iid \\(X= (X_1,...,X_n)\\) où chacune des variables \\(X_i\\) admet la densité \\(f\\) (par rapport à la mesure de Lebesgue). Pour estimer une densité on peut utiliser une méthode à noyau. Les méthodes à noyau sont des méthodes non-paramétriques qui permettent de proposer une estimation de la densité plus lisse que celle obtenue par un histogramme. 2.3.1 Comment construit-on un estimateur à noyau ? Lidée pour la construction de cet estimateur est dutiliser lapproximation suivante , valable lorsque h est petit : \\[ f(x) = F&#39;(x)\\approx \\frac{F(x+h)-F(x-h)}{2h} \\] Pour estimer la densité \\(f\\) on peut passer par un estimateur \\(\\hat F_n\\) de la fonction de répartion \\(F\\). \\(\\hat F_n\\) est la fonction de répartition empirique ( \\(\\hat F_n(x)= \\frac1n \\sum\\limits_{i=1}^n\\frac{1}{2h} \\mathds1_{X_i \\in ]x-h, x+h[}\\) ). \\[ \\hat f_n(x)= \\frac{\\hat F_n(x+h)-\\hat F_n(x-h)}{2h} = \\frac 1n \\sum\\limits_{i=1}^n \\frac1{2h} \\mathds1_{X_i \\in ]x-h;x+h]} \\] Notons \\(\\hat f(x)\\) lestimateur à noyau de la densité \\(f\\), alors celui-ci sécrit : \\[ \\hat f(x) = \\frac1{nh} \\sum\\limits_{i=1}^n K\\left(\\frac{X_i-x}h\\right) \\] où \\(h\\) est la fenêtre (ou paramètre de lissage), \\(n\\) le nombre dobservations, et \\(K\\) le noyau. Cette formule nest valable que si \\(h\\) est petit et positif. ici \\(K(u)= \\frac12 \\mathds{1}_{u \\in ]-1;1]}\\), il sagit du noyau de Rosenblatt, mais il existe dautres noyaux. 2.3.2 Explication ce quest un noyau ? Exemples de noyau : Noyau de Rosenblatt, ou rectangulaire : \\(K(u)= \\frac12 \\mathds{1}_{u\\in]-1;1]}\\) Noyau Gaussien : \\(K(u) =\\frac1{\\sqrt{2\\pi}}exp(-\\frac{u^2}2)\\) Noyau dEpanechnikov : \\(K(u) =\\frac34(1-u^2)\\mathds{1}_{[-1,1]}(u)\\) Noyau triangulaire : \\(K(u) = (1-\\mid u \\mid)\\mathds{1}_{[-1,1]}(u)\\) Noyau Biweight : \\(K(u) = \\frac{15}{16}(1-u^2)^2\\mathds{1}_{[-1,1]}(u)\\) Les propriétés du noyau (continuité, différentiabilité) se transmettent à lestimateur \\(\\hat f_n\\). "],["estimateur-de-densité-à-noyau.html", "Chapter 3 Estimateur de densité à noyau : 3.1 Evaluer un estimateur 3.2 Risque quadratique ponctuel des estimateurs à noyau sur les classe des espaces de Hölder", " Chapter 3 Estimateur de densité à noyau : 3.1 Evaluer un estimateur Avant de commencer cette partie on va dabord introduire quelques notions et définitions Pour évaluer un estimateur on définit le risque associé dun estimateur \\(\\hat f\\) pour lestimateur f.*Pas compris** 3.2 Risque quadratique ponctuel des estimateurs à noyau sur les classe des espaces de Hölder Nous nous intéressons au risque quadratique ponctuel de \\(\\hat{f}_n\\), i.e étant donné \\(x_0 \\in \\mathbb{R}\\) \\[ R(\\hat {f}_n, f) = \\mathbb{E}[|\\hat {f}_n(x_0) - f(x_0)|^2] \\] Rappelons la décomposition biais au carré-variance du risque quadratique: \\[ \\mathbb{E}[|\\hat {f}_n(x_0) - f(x_0)|^2] = (\\mathbb{E}[\\hat {f}_n(x_0)] - f(x_0))^2 + \\mathbb{V}(\\hat {f}_n(x_0)) \\] 3.2.1 Majoration du biais et de la variance Dans cette section, nous allons nous intéresser au compromis biais-variance afin de minimiser le risque quadratique. Les deux propositions suivantes montrent que sous certaines hypothèses, on peut majorer le biais ainsi que la variance. Pour que la variance tende vers zéro, il faut que \\(nh\\) tende vers linfini. En particulier, à \\(n\\) fixé, la variance est une fonction décroissante de \\(h\\). Il y a donc une valeur optimale de \\(h\\) qui doit réaliser léquilibre entre le biais au carrré et la variance. On peut à présent donner un contrôle du risque quadratique par le théorême suivant. \\begin{demo}: On a : \\[ R(\\hat {f}_n(x_0),f(x_0))= \\text{Biais + Variance} \\] Si nous nous référons aux deux propositions précédentes, nous pouvons écrire : \\[ R(\\hat {f}_n(x_0),f(x_0))\\leqslant(\\frac{h^{\\beta}L}{l!}\\int |u|^{\\beta}|K(u)|du)^2 + \\frac{M(\\beta,L)\\begin{Vmatrix}K\\end{Vmatrix}_2^2}{nh} \\] On cherche ensuite la fenêtre \\(h\\) qui minimise cette quantité. Comme on ne se soucie pas vraiment des constantes exactes quand on cherche la vitesse de convergence dun estimateur, on utilisera la notation \\(c_1=(\\frac{L}{l!}\\int |u|^{\\beta}|K(u)|du)^2\\) et \\(c_2=\\frac{M(\\beta,L)\\begin{Vmatrix}K\\end{Vmatrix}_2^2}{nh}\\). On doit alors minimiser en \\(h\\) la quantité : \\[ c_1h^{2\\beta}+\\frac{c_2}{nh} \\] On a une quantité croissante et une quantité décroissante en \\(h\\). Encore une fois, comme on ne se soucie pas pas des constantes, donc on cherche la fenêtre \\(h\\) qui nous donne lordre minimal du risque. Quand \\(h\\) est trop grand, le biais est trop grand, et quand \\(h\\) est trop petit, cest la variance qui est trop grande. On cherche donc la fenêtre \\(h\\) qui réalise un équilibre entre le biais au carré et la variance: \\[ h^{2\\beta}\\approx\\frac{1}{nh} \\] où le signe \\(\\approx\\) signifie ici de lordre de. Cela donne : \\[ h\\approx n^{-\\frac{1}{2\\beta +1}} \\] Autrement dit, pour une fenêtre \\(h\\) de lodre de \\(n^{-\\frac{1}{2\\beta+1}}\\), le biais au carré et la variance sont de même ordre.Plus exactement, on choisit la fenêtre \\(h_*=cn^{-\\frac{1}{2\\beta+1}}\\), avec \\(c\\) une constante positive, on a : \\[ Biais\\ au\\ carré \\approx h_{*}^{2\\beta}\\approx Variance\\approx \\frac{1}{nh_{*}} \\] De plus, on a alors : \\[ h_* \\approx n^{-\\frac{2\\beta}{2\\beta + 1}} \\] Autrement dit, il existe une certaine constante \\(C\\) telle que, pour cette fenêtre \\(h_*\\), on a : \\[ R(\\hat {f}_n(x_0),\\sum_d(\\beta,L))\\leqslant Cn^{\\frac{-2\\beta}{2\\beta + 1}} \\] Cette fenêtre est donc optimale à une constante près (si on change \\(c\\), on change \\(C\\) ça ne change pas le taux qui est \\(n^{\\frac{-2\\beta}{2\\beta+1}}\\)). \\end{demonstration} On a bien trouvé que : \\[R(\\hat{f},f)=biais^2+\\mathbb Var\\] Donc, afin de minimiser lexpression du risque le choix de h est très influent et même plus crucial pour la qualité de lestimateur que celui de la noyau \\(K\\). On doit chercher le meilleur compromis biais-variance pour avoir un risque minimal. Un paramètre trop faible provoque lapparition de détails artificiels sur le graph de lestimateur (La variance devient trop grande), par contre si on prend une valeur de h très grande on aura la majorité des caractéristiques effacées. Bias-variance-trade-off (http://chimix.com/an16/pol16/image/aspts35.jpg) "],["méthodes-adaptatives.html", "Chapter 4 Méthodes adaptatives : 4.1 Choix du noyau 4.2 Choix de la fenêtre", " Chapter 4 Méthodes adaptatives : On a introduit précédemment la notion de lestimation de la densité qui dépend dun paramètre de lissage h. Soit \\((\\hat{f_h})_{h\\in \\mathcal H}\\) une famille des estimateurs de la vrai fonction densité \\(f\\) . La question qui se pose est donc la suivante : comment peut on construire un estimateur à risque optimal à partir de cette famille (en prenant en considération les observations) ? Dans cette partie et afin de repondre a la question quon a poser on va discuter au premier temps du choix du noyau. Ensuite, on va introduire deux méthodes pour le choix du paramètre de lissage h . 4.1 Choix du noyau 4.1.1 Comment choisir les paramètres de la méthode ? Dans la méthode destimation à noyau le choix du noyau nest pas le plus important, le vrai enjeu de cette méthode est le choix de la fenêtre \\(h\\) (bandwidth). En effet, la fenêtre détermine linfluence des données dans lestimation. Si \\(h\\) est petit, leffet local est important donc on aura beaucoup de bruit. Si \\(h\\) est grand on aura une estimation plus douce, plus lisse. Nous pouvons constater linfluence du paramètre \\(h\\) sur lexemple suivant : Nous avons simulé 500 variables suivant une loi de Weibull de paramètres (\\(\\alpha = 1.7\\), \\(\\lambda=2\\)) représentées dans lhistogramme. La courbe en rouge est la vraie fonction de densité et la bleue est lestimation avec la méthode des noyaux sur les variables simulées. par(mfrow=c(1,3)) seq &lt;- seq(0,6, length.out = 40) yweib &lt;- dweibull(seq,1.7,2) rand &lt;-rweibull(500,1.7,2) hist(rand, breaks = 12, freq = F, main = &quot;&quot;) lines(density(rand, bw = 0.1), col =&quot;blue&quot;) lines(density(rand), col =&quot;blue&quot;) lines(density(rand, bw = 2), col =&quot;blue&quot;) lines(seq, yweib, col=&quot;red&quot;) title(&quot;Fenêtre trop petite,\\n h = 0.1&quot;) hist(rand, breaks = 12, freq = F, main = &quot;&quot;) lines(density(rand), col =&quot;blue&quot;) lines(seq, yweib, col=&quot;red&quot;) title(&quot;Fenêtre raisonnable,\\n h = 0.372&quot;) hist(rand, breaks = 12, freq = F, main = &quot;&quot;) lines(density(rand, bw = 2), col =&quot;blue&quot;) lines(seq, yweib, col=&quot;red&quot;) title(&quot;Fenêtre trop petite,\\n h = 2&quot;) La fenêtre \\(h\\) du second graphique est calculé automatiquement par la fonction densityde R. 4.2 Choix de la fenêtre Lestimation de densité nécessite le choix de la fentêtre quon note h.En statistique non-paramétrique, ils éxistent plusieurs méthodes et critéres de qualité pour le choix de la fêntere. On présente dans la suite deux méthodes: Méthode de validation croisée. Méthode de Goldenshluger-Lepski. 4.2.1 Choix de la fenêtre \\(h\\) par validation croisée Le choix de la fenêtre dans la section précédnte est criticable: comme on la mentionné, il dépend de la régularité la fonction \\(f\\) qui est inconnue dans notre cas. On peut donc essayer destimer cette fenêtre idéale par un estimateur \\(\\hat{h}\\). De façon à souligner la dépendance à la fonction, on va noter \\(\\hat{f}_{n,h}\\) lestimateur associé à un choix de fenêtre \\(h\\). Lestimateur final sera \\(\\hat{f}_{n,\\hat{h}}\\), une fois le choix de \\(\\hat{h}\\) fait.*on fait un choix sur h ?** On cherche à minimiser en \\(h\\) le risque quadratique pour la distance \\(L_2\\) : \\[ \\begin{aligned} R(\\hat {f}_{n,h})&amp;=\\mathbb{E}[\\begin{Vmatrix}\\hat {f}_{n,h}-f\\end{Vmatrix}_2^2]\\\\ &amp;= \\mathbb{E}[\\begin{Vmatrix}\\hat {f}_{n,h}\\end{Vmatrix}_2^2] -2~\\mathbb{E}[\\int \\hat {f}_{n,h}(x)f(x)dx] +\\begin{Vmatrix}f\\end{Vmatrix}_2^2 \\end{aligned} \\] Or la fonction \\(f\\) étant inconnue, ce risque nest pas calculable à partir des données. On cherche donc à estimer ce risque en utilisant uniquement les données. Remarquons tout de suite que minimiser en \\(h\\) la quantité \\(R(\\hat {f}_{n,h}, f)\\) est équivalent à minimiser en \\(h\\) la quantité \\(R(\\hat {f}_{n,h}, f)-\\begin{Vmatrix}f\\end{Vmatrix}_2^2\\). On va en fait remplacer la minimisation de la quantité inconnue \\(R(\\hat {f}_{n,h}, f)-\\begin{Vmatrix}f\\end{Vmatrix}_2^2\\) par la minimisation dun estimateur \\(\\hat {R}(h)\\) de cette quantité. Plus précisément on va chercher un estimateur sans biais de cette expression: \\[ \\mathbb{E}[\\begin{Vmatrix}\\hat {f}_{n,h}\\end{Vmatrix}_2^2] -2~\\mathbb{E}[\\int \\hat {f}_{n,h}(x)f(x)dx] \\] Le premier terme admet \\(\\begin{Vmatrix}\\hat {f}_{n,h}\\end{Vmatrix}_2^2\\) comme estimateur trivial (daprès la propriété des estimateurs sans biais : \\(\\mathbb{E}[\\hat {\\beta}]=\\beta\\)). Il reste à trouver un estimateur sans biais du second terme. Pour cela, nous admettons par construction lestimateur sans biais \\(\\hat {G}\\) défini en tout points sauf en \\(X_i\\) (cest le principe du Leave-one-out): \\[ \\hat{G} = \\frac{1}{n}\\sum_{i=1}^n\\hat {f}_{n,h}^{(-i)}(X_i) \\] avec : \\[ \\hat {f}_{n,h}^{(-i)}(x)= \\frac{1}{n-1}\\frac{1}{h}\\sum_{j=1,j\\ne i}^nK(\\frac{x-X_j}{h}) \\] Montrons que \\(\\mathbb{E}(\\hat{G})=\\mathbb{E}[\\int \\hat{f}_{n,h}(x)f(x)dx]\\). Comme les \\(X_i\\) sont i.i.d., dune part nous avons : \\[ \\begin{aligned} \\mathbb{E}[\\int \\hat {f}_{n,h}(x)f(x)dx]&amp;= \\mathbb{E}[\\int \\frac {1}{nh}\\sum_{i=1}^nK(\\frac {x-X_i}{h})f(x)dx]\\\\ &amp;=\\frac{1}{h}\\mathbb{E}[\\int K(\\frac {x-x_1}{h})f(x)dx] \\\\ &amp;=\\frac{1}{h}\\int f(x)\\int K(\\frac {x-X_1}{h})f(x_1)dx_1dx \\end{aligned} \\] Dautre part, nous avons : \\[ \\begin{aligned} \\mathbb{E}[\\hat{G}]&amp;=\\mathbb{E}[\\frac{1}{n}\\sum_{i=1}^n\\hat{f}_{n,h}^{(-i)}(X_i)]\\\\ &amp;=\\mathbb{E}[\\hat{f}_{n,h}^{(-1)}(X_1)]\\\\ &amp;=\\mathbb{E}[\\frac{1}{n(n-1)h}\\sum_{j\\ne 1}K(\\frac{X_j-X_1}{h})]\\\\ &amp;=\\mathbb{E}[\\frac{1}{h}K(\\frac{X-X_1}{h})]\\\\ &amp;=\\frac{1}{h}\\int f(x)\\int K(\\frac{x-x_1}{h})f(x_1)dx_1dx\\\\ &amp;=\\mathbb{E}[\\int \\hat{f}_{n,h}(x)f(x)dx] \\end{aligned} \\] Donc, \\(\\hat{G}\\) est un estimateur sans biais de \\(\\int\\hat{f}_{n,h}(x)f(x)dx\\). Finalement, lestimateur sans biais de \\(R(\\hat{f}_{n,h}, f)-\\begin{Vmatrix}{f}\\end{Vmatrix}_2^2\\) est donné par: \\[ \\hat{R}(h)=\\begin{Vmatrix}\\hat{f}_{n,h}\\end{Vmatrix}_2^2-\\frac{2}{n(n-1)}\\sum_{i=1}\\sum_{j=1,j\\ne i}\\frac{1}{h}K(\\frac{X_i-X_j}{h}) \\] On définit alors \\[ \\hat{h} = arg\\ \\underset{h\\in H}{min}\\hat{R}(h) \\] Si ce minimum est atteint. On cherche une fenêtre parmi une grille finie de valeurs, grille quon a notée \\(H\\) dans la formule ci-dessus. Lestimateur \\(\\hat{f}_{n,\\hat{h}}\\) a de bonnes propriétés pratiques et de consistence. La validation croisée est une méthode très générale mais nous lutilisons ici pour le choix la fenêtre \\(h\\) optimale. 4.2.2 Méthode de Goldenshluger-Lepski La méthode de Goldenshluger-Lepski donne principalement des critères pour le choix entre estimateurs à noyau \\((\\hat{f_h})_{h\\in \\mathcal H}\\) avec différentes fenêtres quon fixe en prennant n considération léchantillon des observations. Cette méthode propose de choisir le \\(\\hat h\\) qui minimise lexpression suivante: \\[ B(h)+V(h) \\] Avec : \\[ B(h)=sup_{h&#39; \\in \\mathcal H}{[\\parallel\\hat f_{h&#39;} - \\hat f _h \\parallel-V(h&#39;)]} \\] Et \\[ V(h)=a \\frac{\\parallel K_{h&#39;}\\parallel^2}{n} \\] Tel que \\(K\\) est le noyau, \\(a\\) un paramètre et \\(V(h)\\) est le terme de pénalisation. On a donc le \\(\\hat h\\) est égale à: \\[ \\hat h = arg min_{h \\in \\mathcal H}(B(h)+V(h)) \\] On sintéresse dans cette méthode à déterminer le terme de pénalisation minimal \\(V(h)\\) tel que si on le dépasse on nobtient plus léquilibre biais-variance. Dans ce cas, la valeur de \\(\\hat h\\) est dordre \\(\\frac{1}{n}\\), Le choix optimal de la fenêtre \\(h\\) suivant cette méthode dans ce cas est \\(n^{-\\frac{1}{2 \\alpha +1}}\\) "],["applications.html", "Chapter 5 Applications 5.1 Fonction dens 5.2 Applications aux données du vivant", " Chapter 5 Applications Rappelons que nous cherchons à estimer la fonction de densité \\(f\\) de la durée avant la création dune nouvelle espèce. Dans ce cadre nous allons faire nos estimations à noyau de la densité sur R en applicant la théorie que nous avons vue jusque là. 5.1 Fonction dens Dans cette partie nous présenterons la fonction dens que nous avons créée en voulant reproduire ce que fait la fonction density de R. Voici son code : Insérer le script de fonc_dens.R 5.2 Applications aux données du vivant Maintenant que nous avons notre fonction dens nous allons pouvoir la comparer à la fonction density de R. Pour les comparer nous allons en profiter pour en même temps les appliquer aux données quon veux étudier. Insérer test noyau.R Estimer loi espérance variance. "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion "],["references.html", "References", " References "]]
