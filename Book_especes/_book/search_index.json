[["intro.html", "Rapport du projet de M1 Combien de temps pour faire une espèce ? Chapitre 1 Introduction 1.1 Motivation 1.2 Problématique", " Rapport du projet de M1 Combien de temps pour faire une espèce ? Wiam Chaoui Sophie Manuel Stéphane Sadio 2021 Chapitre 1 Introduction 1.1 Motivation La classification du vivant est depuis longtemps un vrai casse-tête pour les biologistes, surtout en ce qui concerne la notion despèce. De fait, il existe plusieurs définitions du mot espèce, ce qui rend encore plus compliqué un consensus. Cest pour cela que dans la suite nous ne nous étendrons pas sur cette notion et nous ne nous concentrerons que sur des espèces prédéfinies. Les arbres phylogénétiques sont des outils permettant de représenter graphiquement certaines données de classification. En effet, ils présentent les relations de parenté entre espèces. On retrouve dessous différentes espèces actuelles, mais aussi leurs ancêtres communs (les branchements évolutifs qui correspondent à lapparition dune nouvelle homologie), ou encore la durée avant lapparition dune nouvelle espèce qui est donnée par la longueur des branches. source : (http://zestedesavoir.com/media/galleries/2272/d1a1051e-782b-4b5d-81ac-c5641962b9c8.png.960x960_q85.jpg) Dans la suite, nous nous intéresserons aux branchements évolutifs. Supposons quun branchement évolutif apparaît après une durée aléatoire dune loi fixée \\(\\mu\\) indépendamment du passé et du futur évolutif des espèces. Quelle est cette loi \\(\\mu\\)? Sa variance ? Sa moyenne ? On observe des branchements successifs qui composent larbre phylogénétique et à partir de ces données quantitatives observées, on veut estimer la fonction de densité \\(f\\) qui donne la probabilité quun nouveau branchement évolutif apparaisse après un certain temps. Formellement on a le modèle de densité suivant: soient les variables aléatoires \\(X_1,\\dots,X_n\\), \\(n \\in \\mathbb{N^*}\\) à valeur dans \\(\\mathbb{R}^d\\) (ici \\(d=1\\)), indépendantes et identiquement distribuées de longueurs de branche observées. Elles ont pour fonction de densité \\(f\\) par rapport à la mesure de Lebesgue sur \\(\\mathbb{R}\\) supposée inconnue. Notre objectif est destimer cette fonction densité \\(f\\) sur laquelle on fait le moins dhypothèses possibles. On fera seulement les hypothèses dexistence, de continuité et de positivité de la fonction, en servant une observation \\((X_1,...X_n)\\), ce qui nous mène en statistique non-paramétrique, où le paramètre cherché est une densité de probabilité qui appartient à un espace fonctionnel infini, doù la problématique de notre sujet. 1.2 Problématique Comment estimer la loi de densité de la création dune nouvelle espèce avec une méthode destimation non-paramétrique ? Pour commencer, en se basant sur quelques définitions nous présenterons les méthodes destimations non-paramétriques et en introduisant quelques types destimateurs. Par la suite, nous approfondirons les estimateurs de densité à noyau en menant une discussion sur leurs critères dévaluation. Ainsi nous consacrerons un chapitre pour présenter des méthodes adaptatives. Enfin, pour répondre à la problématique, nous implémenterons un estimateur à noyau adaptatif, avec la méthode de validation croisée puis lutiliserons sur des données darbres phylogénétiques. "],["méthodes-non-paramétriques.html", "Chapitre 2 Méthodes non-paramétriques 2.1 Introduction 2.2 Estimateurs par projection 2.3 Estimateurs à noyau de densité", " Chapitre 2 Méthodes non-paramétriques En statistique, on parle destimation quand on cherche à trouver certains paramètres inconnus caractérisant une distribution à partir dun échantillon de données observées, en se basant sur différentes méthodes. Dans notre cas, le paramètre dintérêt est une fonction de densité appartenant à une classe fonctionnelle non fini-dimensionnelle. On se tourne vers lestimation non-paramétrique lorsque lon traite des paramètres non fini-dimensionnels. Nous présenterons dans la suite une courte introduction à lestimation non-paramétrique et nous introduirons les deux classes principales de lestimation fonctionnelle : lestimation par projection et lestimation à noyau. 2.1 Introduction Lestimation non-paramétrique vise à résoudre des problèmes destimation dans le cadre statistique où le modèle auquel on sintéresse nest pas décrit par un nombre fini de paramètres et dont chacun de ces paramètres ne permet pas de décrire la structure générale de la distribution des variables aléatoires.Cela signifie quon utilise des modèles statistiques à dimension infinie. Dans le cadre de notre problématique on sintéresse à lestimation de densité. Un des principes de base de lestimation de la densité selon une méthode destimation non-paramétrique est le suivant Soit un échantillon \\(X=\\{X_1, \\dots,X_n\\}\\) de variables aléatoires réelles \\(i.i.d.\\) admettant une densité \\(f = F&#39;\\), où \\(F\\) est une fonction de répartition. Supposons que \\(f \\in \\mathcal F\\) où \\(\\mathcal{F}\\) est lespace des fonctions positives, intégrables et dintégrale égale à 1 sur \\({\\mathcal R}\\). On cherche à estimer la fonction de densité inconnue \\(f\\) à partir de ces observations. On notera \\(\\hat f_n\\) lestimateur de \\(f\\). On se trouve donc avec le modèle suivant \\(\\{\\mathbb P=\\mathbb P_f,~f \\in \\mathcal F\\}\\) où \\(\\mathbb P_f\\) est la mesure probabilité de la densité \\(f\\). Lestimation ici concerne donc la fonction elle même plutôt que les paramètres, ce qui explique le nom destimation non-paramétrique. Nous traiterons dans la suite deux grandes familles de méthodes linéaires pour estimer une fonction densité : - lestimation par projection et - lestimation par noyau. 2.2 Estimateurs par projection La méthode destimation par projection se base sur la supposition que la fonction de densité f appartient à lespace de Hilber \\(\\mathcal F=(L^2,||.||,&lt;.,.&gt;)\\). Lidée est quon approxime la fonction de densité f par sa projection orthogonal sur un sous-espace vectoriel de dimension fini. On va décrire dans cette partie la méthodologie de lestimation par projection développer par Birgé et Massart (1993,1998). Dabord on définie ce que cest un estimateur linéaire et la projection orthogonale dune fonction f sur un sous-espace vectoriel de dimension fini. Soit \\(t \\in \\mathbb E_N\\) on a daprès ce qui précède que pour tout \\(j \\in \\{1,..,N\\}\\) \\[ a_j=\\int f ~\\Phi_j = \\mathbb E_f[t(X_1)] \\] où \\(X_1\\) est une observation. On peut estimer lespérance \\(\\mathbb E_f[t(X_1)]\\) par \\(\\frac{1}{n} \\sum_{i=1}^n t(X_1)\\) et on obtient donc le résultat suivant. \\[ ||t||^2 -2&lt;t,s&gt; = ||t||^2-\\frac{2}{n}\\sum_{i=1}^nt(X_1), \\] où \\(t \\in \\mathbb E_N\\) et f la fonction de densité. De plus, \\[ \\mathbb E_f[||t||^2 -2&lt;t,s&gt;] = ||t-s||^2-||s||^2 \\] Ntons par \\(\\phi_n\\) la fonction suivante : Pour tout \\(t\\in \\mathbb E_N\\), \\(\\Theta_n(t)=||t||^2-\\frac{2}{n}\\sum_{i=1}^nt(X_1)\\). On peut estimer $_{E_N}fpar la fonction suivante : \\[ \\hat f = arg~min_{t \\in \\mathbb E_N} \\Theta_n(t) \\] Exemple : lestimation par histogramme Dans la suite on procédera à la méthode la plus fréquemment utilisée pour lestimation dune densité : Lestimation à noyau. (Argument pour le choix de la méthode à voir) 2.3 Estimateurs à noyau de densité Notre but est destimer la densité \\(f\\). Pour cela, on sappuiera sur un échantillon \\(i.i.d.\\) \\(X= (X_1,...,X_n)\\) où chacune des variables \\(X_i\\) admet la densité \\(f\\) (par rapport à la mesure de Lebesgue). Pour estimer une densité on peut utiliser une méthode à noyau. Les méthodes à noyau sont des méthodes non-paramétriques qui permettent de proposer une estimation de la densité plus lisse que celle obtenue par un histogramme. Comment construit-on un estimateur à noyau ? Lidée pour la construction de cet estimateur est dutiliser lapproximation suivante , valable lorsque h est petit : \\[ f(x) = F&#39;(x)\\approx \\frac{F(x+h)-F(x-h)}{2h} \\] Pour estimer la densité \\(f\\) on peut passer par un estimateur \\(\\hat F_n\\) de la fonction de répartition \\(F\\). \\(\\hat F_n\\) est la fonction de répartition empirique ( \\(\\hat F_n(x)= \\frac1n \\sum\\limits_{i=1}^n\\frac{1}{2h} \\mathds1_{X_i \\in ]x-h, x+h[}\\) ). \\[ \\hat f_n(x)= \\frac{\\hat F_n(x+h)-\\hat F_n(x-h)}{2h} = \\frac 1n \\sum\\limits_{i=1}^n \\frac1{2h} \\mathds1_{X_i \\in ]x-h;x+h]} \\] Notons \\(\\hat f(x)\\) lestimateur à noyau de la densité \\(f\\), alors celui-ci sécrit : \\[ \\hat f_n(x) = \\frac1{nh} \\sum\\limits_{i=1}^n K\\left(\\frac{X_i-x}h\\right) \\] où \\(h\\) est la fenêtre (ou paramètre de lissage), \\(n\\) le nombre dobservations, et \\(K\\) le noyau. Cette formule nest valable que si \\(h\\) est petit et strictement positif. ici \\(K(u)= \\frac12 \\mathds{1}_{u \\in ]-1;1]}\\), il sagit du noyau de Rosenblatt, mais il existe dautres noyaux. Exemples de noyau : Noyau de Rosenblatt, ou rectangulaire : \\(K(u)= \\frac12 \\mathds{1}_{u\\in]-1;1]}\\) Noyau Gaussien : \\(K(u) =\\frac1{\\sqrt{2\\pi}}exp(-\\frac{u^2}2)\\) Noyau dEpanechnikov : \\(K(u) =\\frac34(1-u^2)\\mathds{1}_{[-1,1]}(u)\\) Noyau triangulaire : \\(K(u) = (1-\\mid u \\mid)\\mathds{1}_{[-1,1]}(u)\\) Noyau Biweight : \\(K(u) = \\frac{15}{16}(1-u^2)^2\\mathds{1}_{[-1,1]}(u)\\) Les propriétés du noyau (continuité, différentiabilité) se transmettent à lestimateur \\(\\hat f_n\\). Après cette petite présentation de quelques estimateurs non-paramétriques de la densité, nous avons choisi de développer la partie sur lestimation par noyau. cest le type destimation que nous choisirons dutiliser pour la suite de notre étude. "],["estimateur-de-densité-à-noyau.html", "Chapitre 3 Estimateur de densité à noyau 3.1 Risque quadratique ponctuel des estimateurs à noyau sur les classe des espaces de Hölder 3.2 Méthodes adaptatives", " Chapitre 3 Estimateur de densité à noyau Dans un premier temps, nous cherchons à évaluer lestimateur à noyau de densité. Pour cela nous allons utiliser la fonction de risque associée. 3.1 Risque quadratique ponctuel des estimateurs à noyau sur les classe des espaces de Hölder Cette partie a été inspirée par le livre Introduction à la statistique nonparamétrique par Gabriel Turinici. Nous nous intéressons au risque quadratique ponctuel de \\(\\hat{f}_n\\), définit par : étant donné \\(x_0 \\in \\mathbb{R}\\) \\[ R(\\hat {f}_n, f) = \\mathbb{E}[|\\hat {f}_n(x_0) - f(x_0)|^2] \\] Rappelons la décomposition biais-variance du risque quadratique : \\[ \\mathbb{E}[|\\hat {f}_n(x_0) - f(x_0)|^2] = (\\mathbb{E}[\\hat {f}_n(x_0)] - f(x_0))^2 + \\mathbb{V}[\\hat {f}_n(x_0)] \\] 3.1.1 Majoration du biais et de la variance Dans cette section, nous allons nous intéresser au compromis biais-variance afin de minimiser le risque quadratique. Nous introduirons après quelques définitions deux propositions qui montrent que sous certaines hypothèses, on peut majorer le biais ainsi que la variance. Pour que la variance tende vers zéro, il faut que \\(nh\\) tende vers linfini. En particulier, à \\(n\\) fixé, la variance est une fonction décroissante de \\(h\\). Il y a donc une valeur optimale de \\(h\\) qui doit réaliser léquilibre entre le biais au carré et la variance. On peut à présent donner un contrôle du risque quadratique par le théorème suivant. Exemple(Simulation numérique) Nous estimons la fonction densité dune somme de deux variables gaussiennes ci-contre avec la méthode à noyau avec différentes fenêtres. \\[ f(x)=\\frac{1}{2}\\frac{1}{\\sqrt{2\\pi}}(exp(-\\frac{(x-2)^2}{2})+exp(-\\frac{(x-6)^2}{2})) \\] On va en fait utiliser ggplot pour représenter lestimateur à noyau. La fonction qui permet de dessiner lestimateur à noyau est geom_density. Le paramètre représentant le fenêtre h rappelle bw (comme bandwidth en Anglais). 3.2 Méthodes adaptatives On introduit précédemment la notion de lestimation de la densité qui dépend dun paramètre de lissage \\(h\\). Soit \\((\\hat{f_h})_{h\\in \\mathcal H}\\) une famille des estimateurs de la fonction densité \\(f\\) que que lon cherchons cherche à estimer. Une question simpose : comment peut-on construire un estimateur à risque optimal à partir de cette famille en tenant en considération les observations ? Dans la théorie adaptative f est toujours supposée appartenir à une classe fonctionnelle. Cette classe nest pas connu à priori mais supposé appartenir à une famille de classes fonctionnelles{\\(\\mathcal{F_{\\alpha}},\\alpha \\in\\mathcal{A}\\)} où \\(\\mathcal{A}\\) est un ensemble des paramètres de nuisance. ( ref : Sur lestimation adaptative dune densité multivariée sous lhypothèse de la structure dindépendance-Approche minimax adaptative). Dans cette partie, afin de répondre à la question posée auparavant, nous allons discuter du choix du noyau en premier lieu. Ensuite, nous introduirons deux méthodes pour le choix du paramètre de lissage \\(h\\). 3.2.1 Choix du noyau Avant de présenter le critère de choix du noyau nous allons introduire quelques outils mathématiques qui simplifient lécriture du critère. Tout dabord, nous avons besoin du risque quadratique \\(\\mathcal R\\) aussi appelé lerreur quadratique moyenne (Mean Squared Error en anglais). Dans cette partie nous allons noter \\(MSE\\) le risque quadratique pour des questions pratiques. \\[ \\begin{aligned} MSE &amp;= \\mathcal R = \\mathbb E \\left[ \\{ \\hat f_n(x)-f(x) \\} \\right] \\\\ &amp;=\\mathbb V \\left[ \\hat f_n(x) \\right] + Biais^2 \\left[ \\hat f_n(x) \\right] \\\\ &amp;= MSE(x ; n, h, K, f). \\end{aligned} \\] comme nous lavons montré précédemment. @ref{#holder} Lapproximation de lerreur quadratique (Average of Mean Squared Error en anglais) est donnée par léquation suivante : \\[ AMSE(x)= \\frac1{nh} f(x) \\int_{\\mathbb R}K(t)^2 dt + \\left(\\frac12h^2f&#39;&#39;(x)\\int_{\\mathbb R} t^2K(t) dt \\right)^2. \\] Elle a été calculée à partir de la variance approchée et du biais approché. Lerreur quadratique moyenne intégrée (Mean Intregrated Squared Error en anglais) est une mesure théorique communément utilisée pour évaluer la différence entre \\(f\\) et \\(\\hat f_n\\). Pour lévaluer on utilise lerreur quadratique moyenne quon intègre sur le support \\(\\mathbb R\\) de lestimateur. \\[ \\begin{aligned} MISE(n,h,K,f) &amp;= \\int_{\\mathbb R}MSE(x;n,h,K,f)dx\\\\ &amp;= \\int_{\\mathbb R} \\mathbb V\\left[ \\hat f_n(x) \\right] dx+ \\int_{\\mathbb R} Biais^2 \\left[ \\hat f_n(x) \\right]dx \\end{aligned} \\] De la même façon que nous lavons fait avec lerreur quadratique moyenne, nous allons calculer lexpression approchée de lerreur quadratique moyenne (Average of Mean Integrated Squared Error en anglais). \\[ \\begin{aligned} AMISE(x)&amp;= \\frac1{nh} \\int_{\\mathbb R}f(x)dx \\int_{\\mathbb R}K(t)^2 dt + \\frac14h^4 \\int_{\\mathbb R}f&#39;&#39;(x)dx ~~\\left(\\int_{\\mathbb R} t^2K(t) dt \\right)^2\\\\ &amp;= \\frac1{nh} \\int_{\\mathbb R}K(t)^2 dt + \\frac14h^4 \\int_{\\mathbb R}f&#39;&#39;(x)dx ~~\\left(\\int_{\\mathbb R} t^2K(t) dt \\right)^2\\\\ &amp;= \\frac1{nh} \\int_{\\mathbb R}K(t)^2 dt + \\frac14h^4 ~ \\mathbb V (K)^2 \\int_{\\mathbb R}f&#39;&#39;(x)dx \\end{aligned} \\] à présent que nous avons défini les outils nécessaires au choix du noyau, nous allons pouvoir présenter un critère de choix pour les noyaux continus symétriques. Afin de mesurer lefficacité des noyaux, nous utilisons une mesure qui calcule le rapport du critère \\(AMISE\\) de deux noyaux. \\[ eff(K_1,K_2) = \\frac{AMISE(K_1)}{AMISE(K_2)}. \\] Supposons que \\(K_1\\) est le noyau dEpanechnikov, il est souvent utilisé comme référence par rapport aux autres noyaux continus. Après quelques calculs, on obtient que lefficacité dun noyau K par rapport à celui dEpanechnikov est donnée par \\[ eff(K) = \\frac{3}{5\\times \\int_{\\mathbb R}K(t)^2 dt\\sqrt{5 \\times\\int_{\\mathbb R}t^2K(t) dt} } \\leq1. \\] Voici un tableau récapitulatif de lefficacité de plusieurs noyaux continus symétriques. (#tab:ker_tab)Efficacité des noyaux continus symétriques Noyau Efficacite Epanechnikov 1.000 Bigweight 0.994 Triangular 0.986 Gaussien 0.951 Rectangulaire 0.930 Table 3.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa 3.2.1.1 Comment choisir les paramètres de la méthode ? Dans la méthode destimation à noyau le choix du noyau nest pas le plus important, le vrai enjeu de cette méthode est le choix de la fenêtre \\(h\\) (bandwidth). En effet, la fenêtre détermine linfluence des données dans lestimation. Si \\(h\\) est petit, leffet local est important donc on aura beaucoup de bruit. Si \\(h\\) est grand, on aura une estimation plus douce, plus lisse. Nous pouvons constater linfluence du paramètre \\(h\\) sur lexemple suivant : Nous avons simulé 500 variables suivant une loi de Weibull de paramètres (\\(\\alpha = 1.7\\), \\(\\lambda=2\\)) représentées dans lhistogramme. La courbe en rouge est la fonction de densité de la loi de Weibull et la bleue est lestimation avec la méthode des noyaux sur les variables simulées. La fenêtre \\(h\\) du second graphique est calculée automatiquement par la fonction densityde R. 3.2.2 Choix de la fenêtre Lestimation de densité nécessite le choix de la fenêtre quon note h.En statistique non-paramétrique, ils existent plusieurs méthodes et critères de qualité pour le choix de la fenêtre. On présente dans la suite deux méthodes : Méthode de validation croisée. Méthode de Goldenshluger-Lepski. 3.2.2.1 Choix de la fenêtre \\(h\\) par validation croisée Le choix de la fenêtre dans la section précédente est critiquable : comme on la mentionné, il dépend de la régularité la fonction \\(f\\) qui est inconnue dans notre cas. On peut donc essayer destimer cette fenêtre idéale par un estimateur \\(\\hat{h}\\). De façon à souligner la dépendance à la fonction, on va noter \\(\\hat{f}_{n,h}\\) lestimateur associé à un choix de fenêtre \\(h\\). Lestimateur final sera \\(\\hat{f}_{n,\\hat{h}}\\), une fois le choix de \\(\\hat{h}\\) fait. On cherche à minimiser en \\(h\\) le risque quadratique pour la distance \\(L_2\\) : \\[ \\begin{aligned} R(\\hat {f}_{n,h})&amp;=\\mathbb{E}[\\begin{Vmatrix}\\hat {f}_{n,h}-f\\end{Vmatrix}_2^2]\\\\ &amp;= \\mathbb{E}[\\begin{Vmatrix}\\hat {f}_{n,h}\\end{Vmatrix}_2^2] -2~\\mathbb{E}[\\int \\hat {f}_{n,h}(x)f(x)dx] +\\begin{Vmatrix}f\\end{Vmatrix}_2^2 \\end{aligned} \\] Or la fonction \\(f\\) étant inconnue, ce risque nest pas calculable à partir des données. On cherche donc à estimer ce risque en utilisant uniquement les données. Remarquons que minimiser en \\(h\\) la quantité \\(R(\\hat {f}_{n,h}, f)\\) est équivalent à minimiser en \\(h\\) la quantité \\(R(\\hat {f}_{n,h}, f)-\\begin{Vmatrix}f\\end{Vmatrix}_2^2\\). On va en fait remplacer la minimisation de la quantité inconnue \\(R(\\hat {f}_{n,h}, f)-\\begin{Vmatrix}f\\end{Vmatrix}_2^2\\) par la minimisation dun estimateur \\(\\hat {R}(h)\\) de cette quantité. Plus précisément on va chercher un estimateur sans biais de cette expression: \\[ \\mathbb{E}[\\begin{Vmatrix}\\hat {f}_{n,h}\\end{Vmatrix}_2^2] -2~\\mathbb{E}[\\int \\hat {f}_{n,h}(x)f(x)dx] \\] Le premier terme admet \\(\\begin{Vmatrix}\\hat {f}_{n,h}\\end{Vmatrix}_2^2\\) comme estimateur trivial (daprès la propriété des estimateurs sans biais : \\(\\mathbb{E}[\\hat {\\beta}]=\\beta\\)). Il reste à trouver un estimateur sans biais du second terme. Finalement, lestimateur sans biais de \\(R(\\hat{f}_{n,h}, f)-\\begin{Vmatrix}{f}\\end{Vmatrix}_2^2\\) est donné par: \\[ \\hat{R}(h)=\\begin{Vmatrix}\\hat{f}_{n,h}\\end{Vmatrix}_2^2-\\frac{2}{n(n-1)}\\sum_{i=1}\\sum_{j=1,j\\ne i}\\frac{1}{h}K(\\frac{X_i-X_j}{h}) \\] On définit alors \\[ \\hat{h} = arg\\ \\underset{h\\in H}{min}\\hat{R}(h) \\] Si ce minimum est atteint. On cherche une fenêtre parmi une grille finie de valeurs, grille quon a notée \\(H\\) dans la formule ci-dessus. Lestimateur \\(\\hat{f}_{n,\\hat{h}}\\) a de bonnes propriétés pratiques et de consistance. La validation croisée est une méthode très générale mais nous lutilisons ici pour le choix la fenêtre \\(h\\) optimale. Voici un exemple avec validation croisée. "],["applications.html", "Chapitre 4 Applications 4.1 Fonction dens 4.2 Applications aux données du vivant", " Chapitre 4 Applications Rappelons que nous cherchons à estimer la fonction de densité \\(f\\) de la durée avant la création dune nouvelle espèce. Dans ce cadre nous allons faire nos estimations à noyau de la densité sur R en appliquant la théorie que nous avons vue jusque là. 4.1 Fonction dens Dans cette partie nous présenterons la fonction dens que nous avons créée en voulant reproduire ce que fait la fonction density de R. Voici son code : Cette fonction prend en argument léchantillon des observations (sample), le type de noyau en caractère (kernel) et enfin le dernier paramètre est le choix de la fenêtre. Le choix par défaut du noyau est le noyau gaussien et celui de la fenêtre se fait par validation croisée (que nous navons pas codé personnellement). 4.2 Applications aux données du vivant Maintenant que nous avons notre fonction dens nous allons pouvoir la comparer à la fonction density de R. Pour les comparer nous allons en profiter pour en même temps les appliquer aux données quon veut étudier. Pour commencer nous allons tester les fonctions avec un premier exemple fictif. # Estimation de la densité par la méthode des noyaux (Test) ## Chi 2 à 6 ddl seq &lt;- seq(0,30, length.out = 60) ychi2 &lt;- dchisq(seq,6) #plot(seq, ychi2, type = &quot;l&quot;, col = &quot;red&quot;) rand &lt;-rchisq(100,6) hist(rand, breaks = 60, freq = F, xlim=c(0, max(rand)+3)) lines(density(rand, kernel = &quot;epanechnikov&quot;, bw=&quot;ucv&quot;), col =&quot;black&quot;) lines(density(rand, bw=&quot;ucv&quot;),col=&quot;blue&quot;) lines(density(rand, bw=&quot;ucv&quot;, kernel = &quot;rectangular&quot;), col =&quot;purple&quot;) lines(density(rand, bw=&quot;ucv&quot;, kernel = &quot;triangular&quot;), col =&quot;green&quot;) lines(seq, ychi2, col = &quot;red&quot;) legend(&quot;topright&quot;,c(&quot;gaussien&quot;, &quot;epanechnikov&quot;, &quot;rectangulaire&quot;, &quot;triangulaire&quot;, &quot;chi2&quot;), lty=1, col = c(&quot;blue&quot;, &quot;black&quot;,&quot;purple&quot;,&quot;green&quot;,&quot;red&quot;)) rug(rand,col=&quot;darkred&quot;) # Visualisation 1d Comme nous lavons vu théoriquement, on voit relativement bien ici que la différence au niveau du choix du noyau nest pas très impactante sur la qualité de lestimation. Nous allons maintenant tester les fonctions destimation de la densité sur deux exemples. Commençons par les familles doiseaux (bird.families. # Voici des librairies contenant des arbres phylogénétiques. library(ape) library(geiger) ## test avec bird families ### arbre data(&quot;bird.families&quot;) op &lt;- par() par(cex = 0.3) plot(bird.families) par(cex = op$cex) #### noyau gaussien d&lt;-bird.families$edge.length vec &lt;- pretty(0:(max(d)+3),((max(d)+3)*2)) hist(d, breaks = vec, freq = F, xlab = &quot;Durée&quot;) lines(density(d), col = &quot;red&quot;) plot(seq(min(d),max(d), length.out = length(d)), dens(d, kernel= &quot;gaussian&quot;), col = &quot;red&quot;, bty = &quot;n&quot;, xlab = &quot;Durée&quot;, type = &quot;l&quot;, ylim = c(0,0.085)) lines(density(d), col=&quot;blue&quot;) rug(d, col= &quot;red&quot;) legend(&quot;topright&quot;,c(&quot;density&quot;, &quot;dens&quot;), lty=1, col = c(&quot;blue&quot;,&quot;red&quot;)) cat(&quot;La moyenne de cet échantillon est de : &quot;, mean(d), &quot;\\n&quot;) ## La moyenne de cet échantillon est de : 7.413653 cat(&quot;La variance de cet échantillon est de : &quot;, var(d)* (length(d)-1)/length(d)) ## La variance de cet échantillon est de : 38.35033 # estimateur biaisé # max(d) = 27 Sur cette échantillon on remarque une répartition bimodale de leffectif. Effectivement, on observe un mode autour de 1 et un autre autour de 10. On peut supposer en regardant ce tableau que cet échantillon contient des familles doiseaux distinctes, ayant suffisamment de différences pour ne pas avoir le même temps dévolution. Cest pour cela que la moyenne de léchantillon se trouve au final, là où il y a peu dindividus (moins de \\(5\\%\\)). On remarque aussi que les durée sont relativement concentrées avec une variance de \\(38\\), elles ne dépassent pas 27. Maintenant, faisons une autre estimation de la densité sur un échantillon despèces de tortues (chelonia). ### test avec chelonia (turtules) #### arbre data(chelonia) op &lt;- par() par(cex = 0.3) plot(chelonia$phy) par(cex = op$cex) #### noyau gaussien d&lt;-chelonia$phy$edge.length vec &lt;- pretty(0:(max(d)+3), ((max(d)+3)*2)) hist(d, breaks= vec, freq = F, xlab = &quot;Durée&quot;) lines(density(d), col = &quot;red&quot;) plot(seq(min(d),max(d), length.out = length(d)), dens(d), col = &quot;red&quot;, bty = &quot;n&quot;, xlab = &quot;Durée&quot;, type = &quot;l&quot;, ylim=c(0,0.06)) lines(density(d), col=&quot;blue&quot;) rug(d, col= &quot;red&quot;) legend(&quot;topright&quot;,c(&quot;density&quot;, &quot;dens&quot;), lty=1, col = c(&quot;blue&quot;,&quot;red&quot;)) cat(&quot;La moyenne de cet échantillon est de : &quot;, mean(d), &quot;\\n&quot;) ## La moyenne de cet échantillon est de : 12.93598 cat(&quot;La variance de cet échantillon est de : &quot;, var(d)* (length(d)-1)/length(d)) # estimateur biaisé ## La variance de cet échantillon est de : 285.3433 # max(d) = 150 Si on compare cet échantillon avec le précédent, on remarque cette fois la distribution est plus étalée (avec une variance de \\(258.3\\)) et unimodale. Cependant la moyenne nest pas énormément supérieure à celle des familles doiseaux. Cela sexplique par le fait que beaucoup despèces de tortue sont apparues en moins de 25 et quon observe le pic autour de \\(1\\). On en déduit quen moyenne on met plus de temps à obtenir une nouvelle espèce de tortue plutôt quune nouvelle espèce doiseau. La fonction codée se rapproche de density cependant elle est moins performante est définie seulement sur le plus petit intervalle continu contenant toutes les observations. "],["conclusion.html", "Chapitre 5 Conclusion", " Chapitre 5 Conclusion "],["references.html", "Chapitre 6 References", " Chapitre 6 References Lucie Montuelle. Inégalités doracle et mélanges. Statistiques [math.ST]. Université Paris-Sud, 2014. Français. Gilles Rebelles. Sur lestimation adaptative dune densité multivariée sous lhypothèse de la structure dindépendance. Cours-Estimation de densité.https://cedric.cnam.fr/vertigo/Cours/ml/coursEstimationDensite.html "],["references-1.html", "References", " References \\begin{thebibliography}{99} Gilles Rebelles. . \\emph{Cours-Estimation de densité}. &lt;https://cedric.cnam.fr/vertigo/Cours/ml/coursEstimationDensite.html&gt; &lt;https://www.ceremade.dauphine.fr/~turinici/images/stories/work/cours/M1_stat_nonparamP20/stat_nonp_P20_annotations.pdf&gt; http://cmatias.perso.math.cnrs.fr/ Larry Wasserman, , Editeur : Springer-Verlag New York,2006. \\end{thebibliography} "]]
