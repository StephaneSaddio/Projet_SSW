---
author: "Wiam Chaoui        Stéphane Sadio      Sophie Manuel   "
header-includes:
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyhead[L]{Combient de temps pour crée une espèce ?}
  - \fancyhead[R]{\emph{M1 - Biostatistique}}
  - \usepackage{dsfont}
  - \usepackage{subfig}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{amsthm}
  - \usepackage{amsmath}
output:
  pdf_document: 
    toc_depth: 6
    number_sections: yes
    extra_dependencies: dsfont
editor_options: 
  markdown: 
    wrap: 72
---

```{=tex}

\newtheorem{definition}{Définition}
\newtheorem{exemple}{Exemple}
\newtheorem{corollary}{Corollaire}
\newtheorem*{proposition}{Proposition}
\newtheorem{lemma}{Lemme}
\newtheorem*{demonstration}{Démonstration}
\newtheorem{remark}{Remarque}
\newtheorem{propetie}{Propriété}
\newtheorem{theorem}{Théorème}
\newpage
\tableofcontents
\newpage
```

------------------------------------------------------------------------

# Introduction génerale 

\fancyhead[R]{Introduction}

## Motivation

 \hspace*{1cm}
On a *un arbre phylogénétique* présentant les relations de parenté
entre *espèces* et on s'interesse au *branchement évolutif* apparaît
après une durée aleatoire d'une loi fixée $\mu$ indépendamment du passé
et du future évolutifs des espèces .\newline Quelle est cette loi $\mu$
? Sa variance ? Sa moyenne ?.\newline

![Arbre phylogénétique](C:\Users\wiam chaoui\Documents\Projet pfe\images\im)


## Problématique


 \hspace*{1cm}  On obsérve des branchements successives qui composent l'arbre
phylogénétique et a partir de ces données quantitatives obsérvées, on
veut  éstimer la fonction $f$ qui donne le temps qu'il faut pour
qu'un nouveau branchement évolutif apparaisse.\newline    
  Formellement, On a un
échantillon $\chi$={$X_1,...,X_n$} de variables obsérvés qui ont pour
fonction de densité $f$ dans $\mathcal F$ où $\mathcal F$ est un éspace
fonctionnel .On cherche a éstimer cette fontion densité $f$ sur la
quelle on fait le moins d'hypothéses possibles .D'où le modéle suivant
{$P={P_f,f\in \mathcal F}$} qui revient a faire une éstimation
non-paramétrique .\newline Pour répondre a la problématque on cherchera
a comprendre et finalement implémenter un éstimateur à noyau adaptif,
de type Goldenshluger-Lepski sur des données d'arbres phylogénétiques.\newpage




# Méthodes non-paramétriques

\fancyhead[R]{Méthodes non-paramétriques}

  \hspace*{1cm} En statistique, on parle de l'éstimation quand cherche à trouvé certains paramétres inconus caractérisant une distribution a partir d'un échantillon de données observées en se basant sur différentes méthodes.
On se tourne vers l'estimataion non-paramétrique lorsqu'on traite des paramétres à dimension infini. Ce qui est bien notre cas, comme on cherche à éstimer une fonction densité qui apartient au espace fonctionnel.\newline 
 \hspace*{1cm} On présente dans la suite une courte introduction
de l'estimation non paramétrique.On introduit ensuite les deux classes
principales de l'estimation fonctionnelle (l'éstimation par projection
et l'éstimation à noyau) afin de discuter de ces
deux classes et de pourquoi on fait le choix de l'éstimation à noyau.\newline

## Définitions


\begin{definition}
    Èstimation non-paramétrique:\newline
 L'éstimation non-paramétrique vise à résoudre des problémes d'éstmation dans le cadre statistique où le modéle dont on s'intéresse n'est pas décrit par un nombre fini de paramètres et dont chacun de ces paramétres ne permet pas de décrire la structure générale de la distribution des variables aléatoires.\newline Cela signifit qu'on utilise des modéles statistiques à dimension infinis.
\end{definition}


  Dans le cadre de notre problématique on s'interesse a l'estimation de
densité.\newline

\begin{definition}
  Un des proncipes de base de l'éstimation à densité selon une méthode d'éstimation non-paramétrique est le suivant \newline
Pour un échantillon des obsérvations quantitatives $\mathbb {X}$ ={$X_1.....,X_n$} des variables aléatoires i.i.d admettant une densité $f= F'$. Supposons que $f \in \mathcal F$ où
$\mathcal{F}$un espace fonctionnel on cherche à éstimer la fonction
inconnu $f$ à partir de ces observations.\newline On notera $\hat f n$ l'estimateur de f.\newline
On se trouve donc avec le modéle suivant { $\mathbb P$ }{ 
$\mathbb P_f ,f \in \mathcal F$ }, tel que $\mathbb P_f$ est la
mesure probabilté de la densité $f$.
\end{definition}

  L'éstimation ici concerne donc la fonction elle même plutôt que les paramétres, ce qui éxplique le nom de l'éstimation non-paramétrique.\newline


\begin{remark} 
-On notera dans la suite $\hat f$ l'éstimateur de la vrai fonction
$f$.\newline
-On prend souvent les distance $L^p$ avec p =1,2 ou $\infty$ .
\end{remark}


Ils éxiste deux grandes familles de méthodes pour éstimer une fonction densité:\newline
   \hspace*{1cm}  -Èstimation par projection,\newline
   \hspace*{1cm}  -Èstimation par le noyau.

## Èstimateurs par projéction :


\begin{definition}
Èstimation par projection:\newline
Supposant que la fonction $f$ à éstimer est dans l'éspace de Hilbert
$\mathcal F = (L^2 , ||.||, <.,.>)$ avec $(\Phi_j)_{j>0}$ une base orthonormée de $L^2$ ,$\mathbb E _N$  un sous-espace fini de $\mathcal F$ et $1 \leq |N| < \infty$.\newline
De plus $a_{\lambda} = <f,\Phi_{\lambda}> = \int_{\mathbb R}f(x)\Phi_{\lambda}(x) dx$.\newline
Alors, on éstime la fonction $f$ par son projeté 

$$
\Pi_Nf = \sum_{\lambda \in N} a_{\lambda} \Phi_{\lambda}
$$
\end{definition}



\begin{remark}
-Cette méthode nous raméne au cas paramétrique.\newline
\end{remark}



 Dans la suite on procedra à la méthode la plus fréquente utilisée pour l'éstimation d'une densité: L'éstimation par le noyau.\newline 
 (Argument pour le choix de la méthode à voir)

# Èstimateur de densité à noyau :


\fancyhead[R]{Èstimateur de densité à noyau}



## Èvaluer un éstimateur 


Avant de commencer cette partie on va d'abord introduire quelques notions et définitions\newline

\begin{definition}{noyau}
On note par le noyau la fonction intégrable K$\mathbb{R}\rightarrow\mathbb{R}$ tel que:\newline
$\int_{\mathbb{R}}K(u)du =1$ \newline
et soient h<0 le paramétre de lissage.\newline $K_h : u\in \mathbb{R} \rightarrow K(\frac{u}{h})/h$

\end{definition}

\begin{lemma}
On peut approximer la famille $(K_h)_{h>0}$ par l'identité du produit de convolution.
\end{lemma}

\begin{demonstration}
A Faire
\end{demonstration}

\begin{corollary}
$K_h * f : x \rightarrow \int_{\mathbb R} K_h(y-x) f(x) dx$ tend vers la fonction f quand h tend vers 0.(pour la distance $L^2$)
\end{corollary}

Pour évaluer un éstimateur on définit le risque associé d'un éstimateur $\hat f$ pour l'éstimateur f.\newline

\begin{definition} 
La fonction de risque : 
$$ 
\mathcal R(\hat f ,f)=\mathbb E_f[||\hat f -f||^2]
$$
\end{definition}


\begin{remark}
  La fonction de risque associé nous permet de comparer l'éstimateur $\hat f$ et l'éstimation f.\newline
On cherche a ce que ce risque associé soit minimal (i.e tend vers 0 pour un nombre d'obsérvation assez grand.)\newline
\end{remark}


   
### Risque quadratique ponctuel des estimateurs à noyau sur les classe des espaces de Hölder 
   
  Nous nous intéressons au risque quadratique ponctuel de $\hat{f}_n$, i.e étant donné\newline
  $x_0 \in \mathbb{R}$ 

$$
R(\hat {f}_n, f) = \mathbb{E}[|\hat {f}_n(x_0) - f(x_0)|^2]
$$

 Rappelons la décomposition "biais au carré + variance" du risque quadratique:
  
$$
  \mathbb{E}[|\hat {f}_n(x_0) - f(x_0)|^2] = (\mathbb{E}[\hat {f}_n(x_0)] - f(x_0))^2 + \mathbb{V}(\hat {f}_n(x_0))
$$



  
#### Majoration du biais et de la variance.

  Dans cette section, nous allons nous intérresser au compromis biais-variance afin de minimiser le risque quadatique.
Les duex propositions suivantes montrent que sous certaines hypothèses, on peut majorer le biais ainsi que la variance.\newline

\begin{definition} : Soit $l \in \mathbb{N^*}$. On dit que le noyau $K$ est d'ordre $l$ si $u^jK(u)$ est intégrable et $\int u^jK(u)du = 0$, $j = {1,...,l}$.\newline
\end{definition}

\begin{proposition}: Si $f \in \sum(\beta,L)$ avec $\beta >  0$ et $L > 0$ et si $K$ est un noyau d'ordre $l = \left\lfloor{\beta}\right\rfloor$ tel que $\int |{u}^{\beta}||{K(u)}|du < \infty$ alors pour tout $x_0 \in \mathbb{R}$, et pour tout $h>0$ le biais peut être borné comme suit:

$$
|\mathbb{E}[\hat{f}_n(x_0)] - f(x_0)|\leqslant \frac{h^{\beta}L}{l!}\int|u|^{\beta}|K(u)|du
$$
\end{proposition}
  
\begin{demonstration}: (voir Esti-non para.pdf page 97, prop 4.10).\newline

   Le biais au carré tend vers zéro à la vitesse $h^{2\beta}$. Plus la fonction $f$ est régulière, plus le biais tend vite vers zéro quand $h$ tend vers zéro (à condition bien sûr que l'ordre du noyau soit suffisamment grand).\newline
\end{demonstration} 
\begin{proposition}: Si $f$ est bornée et si $K$ est de carré intégrable alors 

$$
\mathbb{V}(\hat {f}_n(x_0)) \leqslant \frac{\begin{Vmatrix}f\end{Vmatrix}_{\infty}\begin{Vmatrix}K\end{Vmatrix}^2_2}{nh}
$$

En particulier, si $f \in \sum(\beta,L)$ alors
$$
\mathbb{V}(\hat{f}_n(x_0))\leqslant\frac{M(\beta, L)}{nh}
$$
\end{proposition}
\begin{demonstration}:\


$$
\mathbb{V}(\hat {f}_n(x_0)) = \mathbb{V}(\frac{1}{nh}\sum_{i=1}^nK(\frac{X_i-x_0}{h})) \newline
=\sum_{i=1}^n\mathbb{V}(\frac{1}{nh}K(\frac{X_i-x_0}{h})) \newline
=\sum_{i=1}^n\mathbb{V}(\frac{1}{nh}K(\frac{X_i-x_0}{h}))  \newline             =\sum_{i=1}^n\frac{1}{n^2h^2}\mathbb{V}(K(\frac{X_i-x_0}{h})) \newline
=\frac{1}{nh^2}\mathbb{V}(K(\frac{X_1-x_0}{h}) \newline
\leqslant \frac{1}{nh^2}\mathbb{E}(K^2(\frac{X_1-x_0}{h})) \newline
=\frac{1}{nh^2}\int K^2(\frac{u-x_0}{h}f(u)du \newline
=\frac{1}{nh}\int K^2(v)f(x_0 +vh)dv
$$ 


Et enfin,on admet le résultat suivant: \newline
il éxiste une constante positive $M(\beta,L)$ tel que $\begin{Vmatrix}f\end{Vmatrix}_{\infty} \leqslant M(\beta, L)$. Ceci implique que 

$$
 \mathbb{V}(\hat {f}_n(x_0))\leqslant\frac{1}{nh}M(\beta, L)\int K^2(v)dv 
$$ 
 \end{demonstration}
 
 Pour que la variance tende vers zéro, il faut que $nh$ tende vers l'infini. En particulier, à $n$ fixé, la variance est une fonction décroissante de $h$. Il y a donc une valeur optimale de $h$ qui doit réaliser l'équilibre entre le biais au carrré et la variance. On peut à présent donner un contrôle du risque quadratique par le théorême suivant.
 
\begin{theorem} Soit $\beta>0$ et $L>0$ et $K$ un noyau de carré intégrable et d'ordre $\left\lfloor{\beta}\right\rfloor$ tel que $\int |u^{\beta}||K(u)|du<\infty$. Alors, en choissant une fenêtre de la forme $h=cn^-\frac{1}{2\beta+1}$ avec une constante $c>0$, on obtient pour tout $x_0 \in \mathbb{R}$,

$$ 
R(\hat {f}_n(x_0)),\sum_d(\beta, L)):= \underset{f\in\sum_d(\beta,L)}{sup}\mathbb{E}[|\hat {f}_n(x_0)-f(x_0)|^2]\leqslant Cn^{-\frac{2\beta}{2\beta+1}}
$$ 
 où $C$ est une constante dépendant de $L,\  \beta, \ c$ et $K$.
 \end{theorem}
\begin{demonstration}: 
  On a

$$
 R(\hat {f}_n(x_0),f(x_0))= \text{Biais + Variance}
$$ 

   Si nous nous référons aux deux propositions précédentes, nous pouvons écrire

$$
 R(\hat {f}_n(x_0),f(x_0))\leqslant(\frac{h^{\beta}L}{l!}\int |u|^{\beta}|K(u)|du)^2 + \frac{M(\beta,L)\begin{Vmatrix}K\end{Vmatrix}_2^2}{nh}
$$

On cherche ensuite la fenêtre $h$ qui minimise cette quantité. Comme on ne se soucie pas vraiment des constantes éxactes quand on cherche la vitesse de convergence d'un estimateur, on utilisera la notation $c_1=(\frac{L}{l!}\int |u|^{\beta}|K(u)|du)^2$ et $c_2=\frac{M(\beta,L)\begin{Vmatrix}K\end{Vmatrix}_2^2}{nh}$. On doit alors minimiser en $h$ la quantité 


$$
  c_1h^{2\beta}+\frac{c_2}{nh}
$$

On a une quantité croissante et une quantité décroissante en $h$. Encore une fois, comme on ne se soucie pas pas des constantes, donc on cherche la fenêtre $h$ qui nous donne l'ordre minimal du risque. Quand $h$ est trop grand, le biais est trop grand, et quand $h$ est trop petit, c'est la variance qui est trop grande. On cherche donc la fenêtre $h$ qui réalise un équilibre entre le biais au carré et la variance:

$$ 
  h^{2\beta}\approx\frac{1}{nh}
$$
où le signe $\approx$ signifie ici "de l'ordre de". Cela donne

$$
  h\approx n^{-\frac{1}{2\beta +1}}
$$

Autrement dit, pour une fenêtre $h$ de l'odre de $n^{-\frac{1}{2\beta+1}}$, le biais au carré et la variance sont de même ordre.Plus exactement, on choisit la fenêtre $h_*=cn^{-\frac{1}{2\beta+1}}$, avec $c$ une constante positive, on a

$$
  Biais\ au\ carré \approx h_{*}^{2\beta}\approx Variance\approx \frac{1}{nh_{*}}
$$

De plus on a alors
$$
  h_* \approx n^{-\frac{2\beta}{2\beta + 1}}
$$

 
Autrement dit, il éxiste une certaine constante $C$ telle que, pour cette fenêtre $h_*$, on a

$$
  R(\hat {f}_n(x_0),\sum_d(\beta,L))\leqslant Cn^{\frac{-2\beta}{2\beta + 1}}
$$

  Cette fenêtre est donc optimale à une constante près (si on change $c$, on change $C$ ça ne change pas le taux qui est $n^{\frac{-2\beta}{2\beta+1}}$).\newline
\end{demonstration}
\begin{remark}: -l'estimatimateur dépend de $\beta$ à travers la fenêtre $h$. Or, sans   connaissance a priori sur les propriétés de la fonction $f$, on ne peut donc pas utiliser cet estimateur. On essaie alors de trouver un choix de fenêtre ne dépendant que des données et qui soit aussi performant (ou presque) que l'estimateur utilisant cette fenêtre optimale. A ce sujet, on introduira plus loin un choix de fenêtr ne dépendant que des données et qui est basé sur ce qu'on appelle la validation croisée (ou "cross validation" en Anglais).\newline
-Nous avons vu plus haut que le biais au carré tend vers zéro quand $h$ tend vers zéro (si $\beta$ est suffisamment grand). Nous en déduisons la convergence de l'espérance de l'estimateur à noyau $\hat {f}_n$ vers la fonction $f$. Et donc, l'estimateur à noyau est asymptôtiquement sans biais, $\hat {f}_n$ est consistante.\newline
\end{remark}




\begin{proposition}

Dans le cas d'un éstimateur à noyau, on a :\newline
$$
R(\hat{f},f)=E_f[||\hat{f}-f||^2 ] = ||f-K_h*f ||^2 + E_f[||\hat{f}-K_h*f ||^2]
$$
\end{proposition}

\begin{demonstration}
On que :\newline 


$E_f||\hat{f}-f ||^2 = \mathbb {E}_f[||\hat{f}+\mathbb {E}_f(\hat{f} )-(\mathbb {E}_f(\hat{f} )-f)|| ]^2$.\newline

$E_f||\hat{f}-f ||]^2 = \mathbb {E}_f||\hat{f}+E_f(\hat{f} )||^2 +\mathbb {E}_f||\mathbb {E}_f(\hat{f} )-f|| ^2 - 2 \mathbb {E}_f(<\hat{f}-\mathbb {E}_f(\hat{f});\mathbb {E}_f(\hat{f})-f>)$.\newline

Comme $\hat{f}$ est déterministe\newline 
$2\mathbb {E}_f(<\hat{f}-E_f(\hat{f});\mathbb {E}_f(\hat{f})-f>)=2<0,E_f(\hat{f})-f>=0$\newline
Ainsi $||\mathbb {E}_f(\hat{f} )-f||$ est déterministe\newline
On obtient :\newline
$R(\hat{f},f)=\mathbb {E}_f[||\hat{f}-f||^2 ] = ||f-K_h*f ||^2 + \mathbb {E}_f[||\hat{f}-K_h*f||^2]$ 
\end{demonstration}

On à bien trouver que\newline
$R(\hat{f},f)=biais^2+\mathbb Var$\newline

\begin{remark}\
   Plus la  valeur de h est  grande,plus le biais devient  grand et la variance petite et vis-vers-ca ; plus la valeur de h est petite plus le  biais devient petit et la variance s'explose.\newline
\end{remark}
 
Donc afin de minimiser l'éxpression de risque le choix  de h est trés influent et même plus crucial pour la qualité de l'éstimateur que celui de la noyau $K$ .\newline
On doit chercher le meilleur compromis biais-variance pour afin avoir un risque minimal.\newline
Un paramétre trop faible provoque l'apparition de détails artificiels sur le graph de l'éstimateur(La variance devient trop grande), par contre si on prend une valeur de h très grande on aura la majorité des caractéristiques éffacée.\newline


 \hspace*{4cm} [![](C:\Users\wiam chaoui\Documents\Projet pfe\Bias-variance-trade-off){width="310"}](http://chimix.com/an16/pol16/image/aspts35.jpg)


## Méthodes adabtatives :

 \hspace*{1cm} On a introduit précedement la notion de l'éstimation de densité qui dépend d'un paramétre de lissage h. Soit  $(\hat{f_h})_{h\in \mathcal H}$ une famille des éstimateurs de la vrai fonction densité $f$ .\newline
 La question qui se pose   est donc la suivante: comment peut on construire un éstimateur à risque optimal à partir de cette famille (en prenant en considération les obsérvations) ? \newline
  \hspace*{1cm}  Dans cette partie et afin de repondre a la question qu'on a poser on va discuter au premier temps du choix du noyau. Ensuite, on va introduir deux méthodes pour le choix du paramétre de lissage h .
 
### Choix du noyau 


### Choix de la fenêtre 
  L'éstimation de densité nécessite le choix de la fentêtre qu'on note h.\newline En statistique non-paramétrique, ils éxistent plusieurs méthodes et critéres de qualité pour le choix de la fêntere.\newline
  
  On présente dans la suite deux méthodes:\newline
   \hspace*{1cm}  Méthode de validation croisée.\newline
   \hspace*{1cm} Méthode de Goldenshluger-Lepski.\newline
    
#### Choix de la fenêtre $h$ par validation croisée
 
 Le choix de la fenêtre dans la section précédnte est criticable: comme on l'a mentionné, il dépend de la régularité la fonction $f$ qui est inconnue dans notre cas. On peut donc essayer d'estimer cette fenêtre idéale par un estimateur $\hat {h}$. De façon à souligner la dépendance à la fonction, on va noter $\hat {f}_{n,h}$ l'estimateur associé à un choix de fenêtre $h$. L'estimateur final sera $\hat{f}_{n,\hat{h}}$, une fois le choix de $\hat{h}$ fait.\newline
 On c herche à minimiser en $h$ la le risque quadratique pour la distance $L_2$:

$$
R(\hat {f}_{n,h})=\mathbb{E}[\begin{Vmatrix}\hat {f}_{n,h}-f\end{Vmatrix}_2^2]\\        \hspace*{1cm}= \mathbb{E}[\begin{Vmatrix}\hat {f}_{n,h}\end{Vmatrix}_2^2] -2\mathbb{E}[\int \hat {f}_{n,h}(x)f(x)dx] +\begin{Vmatrix}f\end{Vmatrix}_2^2
$$

Or la fonction $f$ étant inconnue, ce risque n'est pas calculable à partir des données. On cherche donc à estimer ce risque en utilisant uniquement les données. Remarquons tout de suite que minimiser en $h$ la quantité $R(\hat {f}_{n,h}, f)$ est équivalent à minimiser en $h$ la quantité $R(\hat {f}_{n,h}, f)-\begin{Vmatrix}f\end{Vmatrix}_2^2$. On va en fait remplacer la minimisation de la quantité inconnue $R(\hat {f}_{n,h}, f)-\begin{Vmatrix}f\end{Vmatrix}_2^2$ par la minimisation d'un estimateur $\hat {R}(h)$ de cette quantité. Plus précisément on va chercher un estimateur sans biais de cette expression:


$$
\mathbb{E}[\begin{Vmatrix}\hat {f}_{n,h}\end{Vmatrix}_2^2] -2\mathbb{E}[\int \hat {f}_{n,h}(x)f(x)dx]
$$

Le premier terme admet $\begin{Vmatrix}\hat {f}_{n,h}\end{Vmatrix}_2^2$ comme estimateur trivual (d'après la propriété des estimateurs sans biais: $\mathbb{E}[\hat {\beta}]=\beta)$.\newline
Il reste à trouver un estimateur sans biais du second terme. Pour cela, nous admettons par construction l'estimateur sans biais $\hat {G}$ défini en tout points sauf en $X_i$ (c'est le principe du Leave-one-out):

$$
\hat{G} = \frac{1}{n}\sum_{i=1}^n\hat {f}_{n,h}^{(-i)}(X_i)
$$
avec

$$
  \hat {f}_{n,h}^{(-i)}(x)= \frac{1}{n-1}\frac{1}{h}\sum_{j=1,j\ne i}^nK(\frac{x-X_j}{h})
$$


Montrons que $\mathbb{E}(\hat{G})=\mathbb{E}[\int \hat{f}_{n,h}(x)f(x)dx]$.\newline
Comme les $X_i$ sont i.i.d., d'une part nous avons 
$$
\mathbb{E}[\int \hat {f}_{n,h}(x)f(x)dx]= \mathbb{E}[\int \frac {1}{nh}\sum_{i=1}^nK(\frac {x-X_i}{h})f(x)dx]\\
\hspace*{1cm} =\frac{1}{h}\mathbb{E}[\int K(\frac {x-x_1}{h})f(x)dx] \\
\hspace*{1cm} =\frac{1}{h}\int f(x)\int K(\frac {x-X_1}{h})f(x_1)dx_1dx
$$


D'autre part, nous avons 
$$ 
\mathbb{E}[\hat{G}]=\mathbb{E}[\frac{1}{n}\sum_{i=1}^n\hat{f}_{n,h}^{(-i)}(X_i)]\\
=\mathbb{E}[\hat{f}_{n,h}^{(-1)}(X_1)]\\
=\mathbb{E}[\frac{1}{n(n-1)h}\sum_{j\ne 1}K(\frac{X_j-X_1}{h})]\\
=\mathbb{E}[\frac{1}{h}K(\frac{X-X_1}{h})]\\
=\frac{1}{h}\int f(x)\int K(\frac{x-x_1}{h})f(x_1)dx_1dx\\
=\mathbb{E}[\int \hat{f}_{n,h}(x)f(x)dx] 
$$

Donc, $\hat{G}$ est un estimateur sans biais de $\int\hat{f}_{n,h}(x)f(x)dx$. Finalement, l'estimateur sans biais de $R(\hat{f}_{n,h}, f)-\begin{Vmatrix}{f}\end{Vmatrix}_2^2$ est donné par:

$$
\hat{R}(h)=\begin{Vmatrix}\hat{f}_{n,h}\end{Vmatrix}_2^2-\frac{2}{n(n-1)}\sum_{i=1}\sum_{j=1,j\ne i}\frac{1}{h}K(\frac{X_i-X_j}{h})
$$


On définit alors 
 
$$
\hat{h} = arg\ \underset{h\in H}{min}\hat{R}(h)
$$


Si ce minimum est atteint. On cherche une fenêtre parmi une grille finie de valeurs, grille qu'on a notée $H$ dans la formule ci-dessus.\
L'estimateur $\hat{f}_{n,\hat{h}}$ a de bonnes propriétés pratiques et de consistence.
La validation croisée est une méthode très générale mais nous l'utilisons ici pour le choix la fenêtre $h$ optimale.



#### Méthode de  Goldenshluger-Lepski 


  La méthode de Lepski donne principalement des critéres pour le choix entre éstimateurs à noyau $(\hat{f_h})_{h\in \mathcal H}$ avec différentes fenêtres qu'on fixe en prennat on considération l'échantillon des obsérvations.\newline
  Cette méthode propose de choisir le $\hat h$ qui minimise l'éxpression suivante:
$$
B(h)+V(h)
$$
Avec :

$$
B(h)=sup_{h' \in \mathcal H}{[||\hat f_{h'} - \hat f _h ||-V(h')]}
$$
Et 
$$
V(h)=a \frac{||K_{h'}||^2}{n}
$$
Tel que K est le noyau, $a$ un paramétre et V(h) est le terme de pénalisation.\newline

On a donc le $\hat h$ est égale à:
$$
\hat h = arg min_{h \in \mathcal H}(B(h)+V(h))
$$
\begin{remark}
Le terme de pénalisation choisi est proportionne a la variance de l'éstimateur.
\end{remark}


  On s'interesse dans cette méthode à déterminer le terme de pénalisation minimal V(h) tel que si on le dépasse on n'obtient plus l'équilibre biais-variance.\newline
  
  Dans ce cas, la valeur de $\hat h$ est d'ordre $\frac{1}{n}$,
  
  Le choix optimal de la fenétre h suivant cette méthode dans ce cas est $n^{-\frac{1}{2 \alpha +1}}$