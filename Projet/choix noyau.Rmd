---

output:
  pdf_document: 
    toc: yes
  html_document: default
---

# Choix de la fenetre :

Déja on se ramene a la question du choix de h (choix de la fenetre) parceque a la base on cherchais a minimiser le risque R de l'estimateur $\hat{f}$ pour l'estimation f.\newline
On veut que le risque $\mathcal{R}$ tend vers zero quand le nombre des observation tend vers l'infini \newline
On a pour on introduit deux methodes d'estimation :\newline
Pour la methode a noyau : \newline
Ce  qui rend cette méthode interessante est le fait que la convolution $K_h * f$ tend vers f sur $L^2$ quand h tend vers 0 \newline
On peut donx approximer la vrai densité f par le produit de convolution $K_h *f$ qui satisfait $K_h*f(x)=E_f[k_h(x-X_1)]$. \newline
Ce qui nous raméne a l'expression suivante de l'estimateur a noyau de f pour un paramétre h>0 fixé :\newline
$\hat{f}_h(x)=\frac{1}{n}\sum_1^n K_h(x-X_i)$ \newline

Ensuite ; On a que : $E_f[\hat{f}_h]=K_h*f$  \newline
Et : \newline
$\mathcal{R}(\hat{f}_h,f)=E_f[||\hat{f}_h - f ||^2$ \newline 
De plus \newline
$E_f||\hat{f}-f ||^2 = E_f[||\hat{f}+E_f(\hat{f} )-(E_f(\hat{f} )-f)|| ]^2$ \newline

$E_f||\hat{f}-f ||]^2 = E_f||\hat{f}+E_f(\hat{f} )||^2 +E_f||E_f(\hat{f} )-f|| ^2 - 2 E_f(<\hat{f}-E_f(\hat{f});E_f(\hat{f})-f>)$ \newline
Comme $\hat{f} est$ déterministe : \newline 
$2E_f(<\hat{f}-E_f(\hat{f});E_f(\hat{f})-f>)=2<0,E_f(\hat{f})-f>=0$
Ainsi $||E_f(\hat{f} )-f||$ est déterministe : \newline
On obtient :\newline
$R(\hat{f},f)=E_f[||\hat{f}-f||^2 ] = ||f-K_h*f ||^2 + E_f[||\hat{f}-K_h*f||^2]$ \newline
.\newline
$E_f[||\hat{f}_h - K_h*f||^2] = \int Var_f(\hat{f}_h(x))dx < \frac{1}{n} \int E_f[K^2_h(x-X_1)] dx$ \newline
Et comme $||K_h||^2=||K||^2/h$ \newline
$E_f[||\hat{f}_h - K_h*f||^2]$<$||K||^2/nh$ \newline
Ca nous raméne donc a : \newline
$\mathcal{R}(\hat{f}_h,f) < ||f-K_h*f||^2 =||K||^2/nh$ \newline
On voit donc bien que pour minimiser le risque le biais tend vers 0 quand h tend vers 0 mais plus h est ptit plus la variance devient grande .\newline 
Il fautr donc chercher le meilleur compromis entre ces deux \newline 
Remarque : le choix de h est beaucoup plus crucial que celui de la noyau \newline
Pour répondre a ce probléme on va introduire la méthode de  Goldenshluger-Lepski : \newline 
L'idée est d'estimer le biai du risque avec des fenetres fixées \newline
Soit $(\hat{f}_h)_{h\in \mathcal{H_n}}$ \newline
On va choisir \newline
$\hat{h} = arg min_{h \in \mathcal{H_n}}(A(h) + V(h))$\newline
Avec V est le terme de pénalisation qui estime l'erreur stochastique .\newline
V(h)=$\mathcal{k}' \frac{||K||^2_1 ||K||^2}{nh}$, et A(h) = $max_{h' \in \mathcal{H_n}}(||\hat{f}_{h'} - \hat{f}_{h,h'}||^2 - V(h'))_+$